{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En mi caso tuve problemas conectando los servidores (EN LINUX)!\n",
    "por eso te recomiento ver los sigueintes tutoriales por si tienes problemas al igual que yo!\n",
    "\n",
    "[video](https://youtu.be/H7LiT-MkriU?si=aIDdESCGhiUIU0CK)\n",
    "\n",
    "[pagina](https://medium.com/@abdulhaque.dev/installing-and-configuring-spark-with-mongodb-on-ubuntu-b74a06f9c965) (Para descargar los dos archivos .JAR y meterla dentro de los archivos JARS de la carpeta spark)\n",
    "\n",
    "Ademas de esto! Te tecomiendo ejecutar mongodb-compass, si tu instalacion fue exitosa abrelo desde la consola `mongodb-compass`\n",
    "\n",
    "Ademas de esto, deja dos consolas abiertas con spark activo: `spark-shell`\n",
    "y mongod activo: `sudomongod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 19:31:53 WARN Utils: Your hostname, miguel-9051-900-0098 resolves to a loopback address: 127.0.1.1; using 192.168.1.58 instead (on interface enp6s0)\n",
      "24/05/19 19:31:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/miguel/Descargas/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/miguel/.ivy2/cache\n",
      "The jars for the packages stored in: /home/miguel/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d9b69ff8-6fa6-404f-a3af-404167136969;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 182ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d9b69ff8-6fa6-404f-a3af-404167136969\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/7ms)\n",
      "24/05/19 19:31:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.58:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myProject</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7617acf8a0b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# State the input and output directories of your Database \n",
    "input_uri = \"mongodb://localhost:27017/test.ciudades\"\n",
    "output_uri = \"mongodb://localhost:27017/test.ciudades\"\n",
    "\n",
    "# Make a spark session to use pysaprk and also configure spark to be configured with MongoDB as follows\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"myProject\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", input_uri) \\\n",
    "    .config(\"spark.mongodb.output.uri\", output_uri) \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.2\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  b|  9.4|\n",
      "|  a|  2.7|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('a',3.14), ('b', 9.4), ('a',2.7)]\n",
    "headers = ['id','value']\n",
    "df = spark.createDataFrame(l, headers)\n",
    "n = 3 #\n",
    "df.show(n) # <-- Nos mostrara los primero n elementos del dataframe (por defecto son 20)\n",
    "\n",
    "df.printSchema() # <-- Spark infiere un esquema de datos, que podemos verlo asi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('a',3.14), ('b',True)]\n",
    "headers = ['id','value']\n",
    "## df = spark.createDataFrame(l, headers) \n",
    "## Esta invocación lanzaría una excepción \n",
    "## TypeError: field value: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.BooleanType'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si no le pasamos los parametros del header el los rellenara automaticamente con valores consecutivos _1, _2, _3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  a|  3|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('a',1), ('b', 2), ('a',3)]\n",
    "df = spark.createDataFrame(l)\n",
    "df.show(n)\n",
    "\n",
    "df.printSchema() # <-- Spark sigue infiriendo el esquema de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "l = [('a',3.14), ('b', 9.4), (None, 5.7)]\n",
    "# VAMOS A ASIGNARUN ESQUEMA EN UN DATAFRAME, PARA ESO DEBEMOS CREAR UN OBJETO DE TIPO ** StrucType **\n",
    "# Que contenga tantos objetos StructField como columnas vaya a temner nuestro DataFrame\n",
    "schema = StructType([\n",
    "  #           ESTE ACEPTA 3 PARAMATROS\n",
    "  #           Columna, TIpo de dato, admite valores booleanos?\n",
    "  StructField('id',   StringType(), True),\n",
    "  StructField('value', FloatType(), False)\n",
    "])\n",
    "df = spark.createDataFrame(l, schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "l = [('a',3.14), ('b', 9.4), ('a', True)]\n",
    "schema = StructType([\n",
    "  StructField('id',   StringType(), True),\n",
    "  StructField('value', FloatType(), False)\n",
    "])\n",
    "## df = spark.createDataFrame(l, schema) ## Lanza excepción\n",
    "## TypeError: field value: FloatType can not accept object True in type <class 'bool'>\n",
    "\n",
    "# Error porque solo peude contener valoresde tipo flotante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spak context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  b|  9.4|\n",
      "|  a|  2.7|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = sc.parallelize([('a',3.14), ('b', 9.4), ('a', 2.7)])\n",
    "df = spark.createDataFrame(r, ['id','value'])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Creacion de un DataFrame a partir de un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "| _1| _2| _3|                  _4|    _5| _6| _7| _8|              _9|    _10|_11|_12|\n",
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "|  1|  0|  3|Braund, Mr. Owen ...|  male| 22|  1|  0|       A/5 21171|   7.25|   |  S|\n",
      "|  2|  1|  1|Cumings, Mrs. Joh...|female| 38|  1|  0|        PC 17599|71.2833|C85|  C|\n",
      "|  3|  1|  3|Heikkinen, Miss. ...|female| 26|  0|  0|STON/O2. 3101282|  7.925|   |  S|\n",
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925|     |       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creación de un DataFrame a partir de un RDD\n",
    "import csv\n",
    "\n",
    "raw = (\n",
    "  sc.textFile(\"../../data/Cap7/titanic.csv\") # Leemos el fichero de texto\n",
    "    .map(lambda s: list(csv.reader([s]))[0]) # Dividimos el CSV en listas\n",
    "    .filter(lambda l: l[0] != 'PassengerId') # Eliminamos la cabecera, entonces por defecto quedan numeros...\n",
    ") # <---- ESTE ES EL RDD\n",
    "\n",
    "df = spark.createDataFrame(raw)\n",
    "df.show(3)\n",
    "df.printSchema()\n",
    "headers = ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "df = spark.createDataFrame(raw, headers)\n",
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y escritura de DataFrames desde ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no le pasemos parametros en json printSchema tambein peude recibr parametros como csv!\n",
    "Aunque no hay problema si queda vacia, ya que sprk intentaq predecirlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RT_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n",
      "+--------+--------------+----------------+\n",
      "|RT_count|          text|            user|\n",
      "+--------+--------------+----------------+\n",
      "|       2|   #Tengosueño| {3, Pepe, true}|\n",
      "|      45|  #VivaElLunes|{15, Ana, false}|\n",
      "|     100|¡Gol de Señor!|  {2, Eva, true}|\n",
      "+--------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../../data/Cap7/tweets.json') # aca no le pasamos parametros, y que spark por defecto los interpreta!\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|En el anterior ap...|\n",
      "|y que se crean me...|\n",
      "|antes de poder co...|\n",
      "|un RDD (o varios)...|\n",
      "|Estos RDDs se cre...|\n",
      "|o a partir de fic...|\n",
      "|creará un RDD y s...|\n",
      "|diferentes proces...|\n",
      "|En el anterior ap...|\n",
      "|y que se crean me...|\n",
      "|antes de poder co...|\n",
      "|un RDD (o varios)...|\n",
      "|Estos RDDs se cre...|\n",
      "|o a partir de fic...|\n",
      "|creará un RDD y s...|\n",
      "|diferentes proces...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lee todos los ficheros, y cada línea es una fila del DataFrame.\n",
    "## El DataFrame tiene una única columna 'value' de tipo string\n",
    "df = spark.read.text('../../data/Cap7/*.txt')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|En el anterior ap...|\n",
      "|En el anterior ap...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lee todos los ficheros, y cada ficheros es una fila del DataFrame\n",
    "## El DataFrame tiene una única columna 'value' de tipo string\n",
    "df = spark.read.text('../../data/Cap7/*.txt', wholetext=True)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenaiento de Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escritura de un DataFrame de 50 elementos en la carpeta /tmp/csv\n",
    "l = [('a',3.14)] * 50\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.printSchema()\n",
    "df.write.csv('/tmp/csv', header=True, mode='overwrite') # recordemos quese guardara al inicio de nuestro disco!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si queiremos cambiar el parametro de separacion podemosa usar `sep` y para incluir las cabeceras en cada fichero utilizariamos `header=True`\n",
    "\n",
    "Otro paraetro interesante es el modo de escritura `mode`, que nos permitira congfgurar que hacer si la carpeta ya existe. Este acepta 4 valores, `append` que agregara los datos a los ficheros ya existentes, `overwrite` los sobreescribiremos completramente, `ignore` evitaremos cualquier escritura si existen ya ficheros, y `error` lanzaremos una excepecion si la carpeta ya existia anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El metodo spark.read nos permite pasar como parametro una carpeta, por lo cual no sera un problema quese escriban varios ficheros al tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del DataFrame anterior desde la carpeta /tmp/csv\n",
    "df = spark.read.csv('/tmp/csv', header=True, inferSchema=True) # todos los elmentos de esa carpeta\n",
    "df.printSchema()\n",
    "df.show(4)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "l = [('a',3.14)] * 50\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.printSchema()\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.json('/tmp/json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del DataFrame anterior desde la carpeta /tmp/json\n",
    "df = spark.read.json('/tmp/json') # Aqui no debemos\n",
    "df.printSchema()\n",
    "df.show(4)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un Dataframe de Pandas a partir de un Dataframe de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install distutils\n",
    "# Volcar un DataFrame a Pandas\n",
    "l = [('a',3.14)] * 12\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df_pandas = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames y MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder cargar y salvar DataFrames en MongoDB es necesario lanzar ```pyspark``` configurando el conector de Mongo-Spark. Para ello es necesario pasar el siguiente parámetro a la hora de invocar a ```pyspark```:\n",
    "\n",
    "Iiciar mongo:\n",
    "```shell\n",
    "sudo systemctl start mongod\n",
    "```\n",
    "\n",
    "verifica que esta activo:\n",
    "\n",
    "```shell\n",
    "sudo systemctl status mongod\n",
    "```\n",
    "\n",
    "empieza a usarlo:\n",
    "\n",
    "```shell\n",
    "mongosh\n",
    "```\n",
    "\n",
    "si acabas peude finaizar con:\n",
    "\n",
    "```shell\n",
    "sudo systemctl stop mongod\n",
    "```\n",
    "\n",
    "\n",
    "mongod db documentacion: [aqui](https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-ubuntu/)\n",
    "\n",
    "\n",
    "```\n",
    "pyspark --packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.2 --master local[*]\n",
    "```\n",
    "        \n",
    "El nombre del conector corresponde a _coordenadas_ Maven:\n",
    " * **2.11** porque el conector utiliza la versión 2.11 de Scala\n",
    " * **2.2.2** es la versión concreta del conector. La versión 2.2.2 soporta MongoDB 2.2.x y 2.3.x\n",
    " \n",
    "El parámetro ```--master``` es el usual. En este caso lanzamos ```pyspark``` en modo local y utilizando tantos procesos _workers_ como núcleos tenga nuestra CPU.\n",
    "\n",
    "Se puede encontrar más información en:\n",
    " * https://docs.mongodb.com/spark-connector/master/\n",
    " * https://docs.mongodb.com/spark-connector/master/python-api/\n",
    " \n",
    "**Supondremos que existe un servidor MongoDB ejecutándose en local (IP 127.0.0.1) y escuchando en el puerto por defecto (27017).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|   nombre|habitantes|\n",
      "+---------+----------+\n",
      "|   Madrid|   3182981|\n",
      "|Barcelona|   1620809|\n",
      "| Valencia|    787808|\n",
      "|  Sevilla|    689434|\n",
      "| Zaragoza|    664938|\n",
      "|   Málaga|    569002|\n",
      "|   Murcia|    443243|\n",
      "|    Palma|    406492|\n",
      "+---------+----------+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- habitantes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Volcar un DataFrame a una colección MongoDB\n",
    "\n",
    "ciudades = spark.createDataFrame([(\"Madrid\",  3182981), (\"Barcelona\", 1620809), (\"Valencia\",787808),\n",
    "                                  (\"Sevilla\", 689434), (\"Zaragoza\", 664938), (\"Málaga\",569002), \n",
    "                                  (\"Murcia\",443243), (\"Palma\",406492) ], [\"nombre\", \"habitantes\"])\n",
    "ciudades.show()\n",
    "ciudades.printSchema()\n",
    "\n",
    "ciudades.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()\n",
    "\n",
    "# *Importante*: La colección test.ciudades no debe existir. Si lo que queremos es añadir a una colección ya existente, debemos usar mode(\"append\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|                 _id|habitantes|   nombre|\n",
      "+--------------------+----------+---------+\n",
      "|{664230d2b7e3841e...|   1620809|Barcelona|\n",
      "|{664230d2b7e3841e...|   3182981|   Madrid|\n",
      "|{664230d2b7e3841e...|    406492|    Palma|\n",
      "|{664230d2b7e3841e...|    664938| Zaragoza|\n",
      "|{664230d2b7e3841e...|    443243|   Murcia|\n",
      "|{664230d2b7e3841e...|    689434|  Sevilla|\n",
      "|{664230d2b7e3841e...|    569002|   Málaga|\n",
      "|{664230d2b7e3841e...|    787808| Valencia|\n",
      "|{66423252b7e3841e...|   1620809|Barcelona|\n",
      "|{66423252b7e3841e...|    406492|    Palma|\n",
      "|{66423252b7e3841e...|    787808| Valencia|\n",
      "|{66423252b7e3841e...|    664938| Zaragoza|\n",
      "|{66423252b7e3841e...|    443243|   Murcia|\n",
      "|{66423252b7e3841e...|    569002|   Málaga|\n",
      "|{66423252b7e3841e...|    689434|  Sevilla|\n",
      "|{66423252b7e3841e...|   3182981|   Madrid|\n",
      "|{664296b7e0e1be44...|    664938| Zaragoza|\n",
      "|{664296b7e0e1be44...|    443243|   Murcia|\n",
      "|{664296b7e0e1be44...|   1620809|Barcelona|\n",
      "|{664296b7e0e1be44...|    406492|    Palma|\n",
      "+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- habitantes: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar un DataFrame desde una colección MongoDB\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\").load()\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vams a pasar todos las ciudades con mas de 500000 habitantes!\n",
    "\n",
    "Esto lo hacemos gracias a que usamos las tuberias (pipelines) que usamos con mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|                 _id|habitantes|   nombre|\n",
      "+--------------------+----------+---------+\n",
      "|{664230d2b7e3841e...|   1620809|Barcelona|\n",
      "|{664230d2b7e3841e...|   3182981|   Madrid|\n",
      "|{664230d2b7e3841e...|    664938| Zaragoza|\n",
      "|{664230d2b7e3841e...|    689434|  Sevilla|\n",
      "|{664230d2b7e3841e...|    569002|   Málaga|\n",
      "|{664230d2b7e3841e...|    787808| Valencia|\n",
      "|{66423252b7e3841e...|   1620809|Barcelona|\n",
      "|{66423252b7e3841e...|    787808| Valencia|\n",
      "|{66423252b7e3841e...|    664938| Zaragoza|\n",
      "|{66423252b7e3841e...|    569002|   Málaga|\n",
      "|{66423252b7e3841e...|    689434|  Sevilla|\n",
      "|{66423252b7e3841e...|   3182981|   Madrid|\n",
      "|{664296b7e0e1be44...|    664938| Zaragoza|\n",
      "|{664296b7e0e1be44...|   1620809|Barcelona|\n",
      "|{664296b7e0e1be44...|    787808| Valencia|\n",
      "|{664296b7e0e1be44...|    569002|   Málaga|\n",
      "|{664296b7e0e1be44...|    689434|  Sevilla|\n",
      "|{664296b7e0e1be44...|   3182981|   Madrid|\n",
      "|{66429c19e0e1be44...|    664938| Zaragoza|\n",
      "|{66429c19e0e1be44...|    569002|   Málaga|\n",
      "+--------------------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = \"{'$match': {'habitantes': {$gt:500000}}}\"\n",
    "masde500mil = (spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "                        .option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\")\n",
    "                        .option(\"pipeline\", pipeline).load()\n",
    "              )\n",
    "masde500mil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----+------+\n",
      "|                 _id|altura|edad|nombre|\n",
      "+--------------------+------+----+------+\n",
      "|{664a9a08e67c9aa9...|  NULL|  33|   ana|\n",
      "|{664a9a08e67c9aa9...|   150|NULL| pedro|\n",
      "+--------------------+------+----+------+\n",
      "\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- altura: integer (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar un DataFrame desde una colección MongoDB heterogénea (distintos documentos tienen atributos diferentes)\n",
    "from pymongo import MongoClient #pip install pymongo\n",
    "\n",
    "# Borramos la colección test.usuarios e insertamos dos documentos heterogéneos\n",
    "client = MongoClient('127.0.0.1') # local host\n",
    "col = client['test']['usuarios']\n",
    "col.drop()\n",
    "col.insert_many([{'nombre':'ana', 'edad':33}, {'nombre':'pedro','altura':150}]) # insertamos dos nuevs elementos (insert_many)\n",
    "\n",
    "# Cargamos un DataFrame a partir de la colección test.usuarios\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.usuarios\").load()# en la tabla test de la coleccion usuarios se guardaran los datos\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  b|  2.0|\n",
      "|  c|  4.5|\n",
      "+---+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "l = [('a',3.14), ('b',2.0), ('c',4.5)]\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "print(df.count()) # muestra las filas del dataframe\n",
    "\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metodo describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|summary|  id|value|\n",
      "+-------+----+-----+\n",
      "|  count|  12|   12|\n",
      "|   mean|NULL| 3.14|\n",
      "| stddev|NULL|  0.0|\n",
      "|    min|   a| 3.14|\n",
      "|    max|   a| 3.14|\n",
      "+-------+----+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "l = [('a',3.14)] * 12\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.describe().show()\n",
    "type(df.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a analizar los datos del titanic con el metodo `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/19 19:32:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------+-----+\n",
      "|summary|               Age|             Fare|   Sex|Cabin|\n",
      "+-------+------------------+-----------------+------+-----+\n",
      "|  count|               714|              891|   891|  204|\n",
      "|   mean| 29.69911764705882| 32.2042079685746|  NULL| NULL|\n",
      "| stddev|14.526497332334035|49.69342859718089|  NULL| NULL|\n",
      "|    min|              0.42|              0.0|female|  A10|\n",
      "|    max|              80.0|         512.3292|  male|    T|\n",
      "+-------+------------------+-----------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "#### EL METODO DESCRIBE ACEPTA VARIOS PARAMETROS ####\n",
    "df.describe(['Age','Fare','Sex','Cabin']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminación de columnas con drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']  = Columnas seleccionadas\n",
      "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']  = eliminacion de 3 columnas\n",
      "['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']  = eliminacion de la columna 'PassegerId'\n",
      "['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']  = eliminacion de la columna 'PassegerId'\n"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.columns, \" = Columnas seleccionadas\")\n",
    "df2 = titanic.drop('PassengerId','Name','Cabin')\n",
    "print(df2.columns, \" = eliminacion de 3 columnas\")\n",
    "df3 = titanic.drop(titanic.PassengerId)\n",
    "print(df3.columns, \" = eliminacion de la columna 'PassegerId'\")\n",
    "df4 = titanic.drop(titanic['PassengerId'])\n",
    "print(df4.columns, \" = eliminacion de la columna 'PassegerId'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de columnas con select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "['Survived', 'Pclass', 'Age']\n"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.columns)\n",
    "df2 = titanic.select('Survived', 'Pclass', 'Age') # seleccionamos estas columnas y eliminamos las otras :)\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seleccion de filas con dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "891\n"
     ]
    }
   ],
   "source": [
    "#este metodo nos permite eiminar los duplicados, podemos dejarlo vacio y tomara en cuent todas las columnas, aunque podemos pasarle parametros utiles\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.count())\n",
    "df = titanic.dropDuplicates()\n",
    "print(df.count())\n",
    "#no se elimino ningun elemento porque todos son unicos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "en este caso dropDUplicates() eliminara todos los valores menos los dos unicosvalores (Female Y Male...) \n",
    "\n",
    "Es decir, quedaran unicamente dos valores..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "df = titanic.dropDuplicates(['Sex'])\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descarte por dropna\n",
    "\n",
    "Dispone de un prametro `how`, que indica que se debn eliminar las filas que tengan todos sus valores nulos (valor 'all') o si unicamente es necesario que contenga algun valo vacio para proceder a su eliminacion (valor 'any')\n",
    "\n",
    "Tambien prmite definir cual es el minimo de numero de vaores no vacios que debe tener un fila para conservarla en el DataFrame (parametro thresh), este paramaetro invalidara el valor pasado por `how`\n",
    "\n",
    "Tambien nospermite pasaruna list de nombres de columnas sobre las que buscar valores vacios en su parametro `subset`\n",
    "\n",
    "\n",
    "Por defecto `dropna` eliminara aquellas filas que tengan un valor vacio en alguna de sus clumnas `how='any'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "183\n",
      "712\n"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.count())\n",
    "df = titanic.dropna() # elimina todos los que tegan al menos un valor vacio!\n",
    "print(df.count()) #quedan unicamente 183 elementos\n",
    "df = titanic.drop('Cabin').dropna() # elimina la columna Cabin, para luego eiminar los objetos vacios (no se elimnaran tatos)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operacion Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n",
      "342\n"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "df = titanic.filter( 'Survived = 1') #sintaxis sql\n",
    "print(df.count())\n",
    "df = titanic.filter( df.Survived == 1) # esto hara que nos pase todos los sobrevivientes!\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "las condiciones se pueden combinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "df = titanic.filter( 'Survived = 1 AND Sex = \"female\" AND Age > 20') # las mujeres de mas de 20 a;os!\n",
    "print(df.count())\n",
    "df = titanic.filter( (df.Survived == 1) & (df['Sex'] == 'female') & (df.Age > 20))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Filas cuyo camarote tiene 3 caracteres, el primero es una A o una B mayúsculas y el último un 2\n",
    "# Usa una expresión regular\n",
    "df = titanic.filter('Cabin RLIKE \"^[AB].2$\"')\n",
    "print(df.count())\n",
    "df = titanic.filter(df.Cabin.rlike('^[AB].2$'))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinación de DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la combinacion de columnas los Dataframes deben ser identicos en su numeros de filas y en sus tipos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para unir dos Dataframes podemos usar el metodo `union`:\n",
    "\n",
    "Union no elimina duplicados... es decir que si en alguno de los dos dataframes los elementos se repiten estaran duplicados al unirlos, pero igualmente podemos eliminarlos con `dropduplicates` como vimos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "|  3| marta|\n",
      "|  1|   ana|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.union(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|1.0|   ana|\n",
      "|2.0|  jose|\n",
      "|3.0| marta|\n",
      "|1.0|   ana|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(3.0,'marta'),(1.0,'ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "df = df1.union(df2) ## No hay problema: long y double son compatibles => double\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "|  3| marta|\n",
      "|  1|   ana|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([('3','marta'),('1','ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "df = df1.union(df2) ## No hay problema: long y string son compatibles => string\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: boolean (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(True,'marta'),(False,'ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "## df = df1.union(df2) ## Excepcion Por tipo de datos\n",
    "##AnalysisException: \"Union can only be performed on tables with the compatible column types. boolean <> bigint at the first column of the second table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(3,'marta',33),(4,'señor',44)],['id','nombre','edad'])\n",
    "df2.printSchema()\n",
    "## df = df1.union(df2)  ## Excepción Por tipo de datos\n",
    "## AnalysisException: \"Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.intersect(df2) # \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  2|  jose|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.subtract(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodo join\n",
    "combina datos de dos DataFrames fusionado las filas que tienen los mismo valroes en coertas columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "+---+------+\n",
      "\n",
      "+---+----+\n",
      "| id|edad|\n",
      "+---+----+\n",
      "|  1|  36|\n",
      "|  2|  30|\n",
      "+---+----+\n",
      "\n",
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   ana|  36|\n",
      "|  2|  jose|  30|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## inner join de dos DataFrames sobre la columna 'id'\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.show()\n",
    "df = users.join(age,'id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto funcionara siempre yu cuando los tipos de datos sean compatibles entre si\n",
    "\n",
    "En este ejemplo se guardara el id como texto y como entero:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join donde la columna 'id' contiene enteros y cadenas de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- edad: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: long (nullable = true)\n",
      "\n",
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   ana|  36|\n",
      "|  2|  jose|  30|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.createDataFrame([('1','ana'),('2','jose')],['id','nombre'])\n",
    "users.printSchema()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.printSchema()\n",
    "df = users.join(age,'id') ## No hay problema: string y long son compatibles => string\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inner join considerando igualdad de varias columnas a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "| id|nombre|deporte|\n",
      "+---+------+-------+\n",
      "|  1|   ana|   golf|\n",
      "|  2|  jose|   polo|\n",
      "+---+------+-------+\n",
      "\n",
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   eva|  33|\n",
      "|  2|  jose|  30|\n",
      "+---+------+----+\n",
      "\n",
      "+---+------+-------+----+\n",
      "| id|nombre|deporte|edad|\n",
      "+---+------+-------+----+\n",
      "|  2|  jose|   polo|  30|\n",
      "+---+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.createDataFrame([(1,'ana','golf'),(2,'jose','polo',)],['id','nombre','deporte'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,'eva',33),(2,'jose',30)],['id','nombre','edad'])\n",
    "age.show()\n",
    "df = users.join(age,[\"id\", \"nombre\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inner join usando una expresión de igualdad entre 2 columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "+---+------+\n",
      "\n",
      "+-----+----+\n",
      "|ident|edad|\n",
      "+-----+----+\n",
      "|    1|  36|\n",
      "|    2|  30|\n",
      "+-----+----+\n",
      "\n",
      "+---+------+-----+----+\n",
      "| id|nombre|ident|edad|\n",
      "+---+------+-----+----+\n",
      "|  1|   ana|    1|  36|\n",
      "|  2|  jose|    2|  30|\n",
      "+---+------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['ident','edad'])\n",
    "age.show()\n",
    "df = users.join(age,users.id == age.ident)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inner join usando una expresión de igualdad entre 4 columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|sport|\n",
      "+---+----+-----+\n",
      "|  1| ana| golf|\n",
      "|  2|jose| polo|\n",
      "+---+----+-----+\n",
      "\n",
      "+-----+------+----+\n",
      "|ident|nombre|edad|\n",
      "+-----+------+----+\n",
      "|    1|   eva|  33|\n",
      "|    2|  jose|  30|\n",
      "+-----+------+----+\n",
      "\n",
      "+---+----+-----+-----+------+----+\n",
      "| id|name|sport|ident|nombre|edad|\n",
      "+---+----+-----+-----+------+----+\n",
      "|  2|jose| polo|    2|  jose|  30|\n",
      "+---+----+-----+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.createDataFrame([(1,'ana','golf'),(2,'jose','polo',)],['id','name','sport'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,'eva',33),(2,'jose',30)],['ident','nombre','edad'])\n",
    "age.show()\n",
    "df = users.join(age,(users.id == age.ident) & (users.name == age.nombre))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inner join usando una expresión compleja entre 2 columnas\n",
    "En este ejemplo vamos a tomar el segundfo valor si el primero es vacio, o tomamos el primero si el primero no es vacio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "+---+------+\n",
      "\n",
      "+----+---+----+\n",
      "| id1|id2|edad|\n",
      "+----+---+----+\n",
      "|NULL|  1|  33|\n",
      "|   2|  5|  30|\n",
      "+----+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---+----+\n",
      "| id|nombre| id1|id2|edad|\n",
      "+---+------+----+---+----+\n",
      "|  1|   ana|NULL|  1|  33|\n",
      "|  2|  jose|   2|  5|  30|\n",
      "+---+------+----+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(None,1,33),(2,5,30)],['id1','id2','edad'])\n",
    "age.show()\n",
    "cond = ((age.id1.isNotNull() & (users.id == age.id1)) | \n",
    "       (age.id1.isNull() & (users.id == age.id2)))\n",
    "df = users.join(age,cond)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "|  3|   eva|\n",
      "+---+------+\n",
      "\n",
      "+---+----+\n",
      "| id|edad|\n",
      "+---+----+\n",
      "|  1|  36|\n",
      "|  2|  30|\n",
      "+---+----+\n",
      "\n",
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   ana|  36|\n",
      "|  2|  jose|  30|\n",
      "|  3|   eva|NULL|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## left outer join de users y age\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose'),(3,'eva')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.show()\n",
    "df = users.join(age,'id','left_outer')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_titanic():\n",
    "    return spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True).drop('Cabin').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operaciones aritméticas entre columnas\n",
    "\n",
    "EN este caso vamos a dejar unicamente 3 columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+\n",
      "|Survived|Family|  Age|\n",
      "+--------+------+-----+\n",
      "|       0|     1|264.0|\n",
      "|       1|     1|456.0|\n",
      "|       1|     0|312.0|\n",
      "+--------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "titanic = load_titanic()\n",
    "titanic.selectExpr(\"Survived\",\"SibSp + Parch As Family\", \"Age * 12 AS Age\").show(3)\n",
    "#                  \\__-> COLUMNA 1  \\__-> Al sumar las dos sale una  \\__--> Al operar Age * 12 creamos una nueva tabla!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|   Sex|Sex_num|\n",
      "+------+-------+\n",
      "|  male|      1|\n",
      "|female|      0|\n",
      "|female|      0|\n",
      "+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "def sex_to_num(s):\n",
    "    ret = None\n",
    "    if s == 'female':\n",
    "        ret = 0\n",
    "    elif s == 'male':\n",
    "        ret = 1\n",
    "    return ret\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark.udf.register(\"sex_to_num\", sex_to_num, IntegerType())\n",
    "\n",
    "titanic.selectExpr(\"Sex\", \"sex_to_num(Sex) AS Sex_num\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+\n",
      "|SibSp|Parch|Max_Family|\n",
      "+-----+-----+----------+\n",
      "|    1|    0|         1|\n",
      "|    1|    0|         1|\n",
      "|    0|    0|         0|\n",
      "+-----+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "spark.udf.register(\"max_int\", max, IntegerType())\n",
    "    \n",
    "titanic.selectExpr(\"SibSp\", \"Parch\", \"max_int(SibSp, Parch) AS Max_Family\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "| Age|         Scaled_Age|\n",
      "+----+-------------------+\n",
      "|22.0| 0.2711736617240513|\n",
      "|38.0| 0.4722292033174164|\n",
      "|26.0|0.32143754712239253|\n",
      "+----+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "def scale(n,minv,maxv):\n",
    "    return (n - minv) / (maxv - minv)\n",
    "\n",
    "# Se puede obtener los valores minimos y maximos de cada columna a través del DataFrame generado por describe()\n",
    "summary = titanic.describe().toPandas()\n",
    "min_age = float(summary.loc[3,'Age'])\n",
    "max_age = float(summary.loc[4,'Age'])\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "spark.udf.register(\"scale_Age\", lambda x: scale(x, min_age, max_age), DoubleType())\n",
    "\n",
    "titanic.selectExpr(\"Age\", \"scale_Age(Age) AS Scaled_Age\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+\n",
      "|Survived|Family|  Age|\n",
      "+--------+------+-----+\n",
      "|       0|     1|264.0|\n",
      "|       1|     1|456.0|\n",
      "|       1|     0|312.0|\n",
      "+--------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformación de columnas usando expresiones entre columnas\n",
    "titanic = load_titanic()\n",
    "titanic.select(titanic.Survived,(titanic.SibSp + titanic.Parch).alias(\"Family\"), (titanic.Age * 12).alias(\"Age\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+\n",
      "|         Scaled_Age|Sex_Num|Max_Family|\n",
      "+-------------------+-------+----------+\n",
      "| 0.2711736617240513|      1|         1|\n",
      "| 0.4722292033174164|      0|         1|\n",
      "|0.32143754712239253|      0|         0|\n",
      "+-------------------+-------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformaciones con UDFs usando expresiones entre columnas\n",
    "from pyspark.sql.functions import udf\n",
    "titanic = load_titanic()\n",
    "\n",
    "sex_to_num_UDF = udf(sex_to_num, IntegerType())\n",
    "\n",
    "max_int_UDF = udf(max, IntegerType())\n",
    "\n",
    "summary = titanic.describe().toPandas()\n",
    "min_age = float(summary.loc[3,'Age'])\n",
    "max_age = float(summary.loc[4,'Age'])\n",
    "scale_Age_UDF = udf(lambda x : scale(x, min_age, max_age), DoubleType())\n",
    "                   \n",
    "titanic.select(scale_Age_UDF(titanic.Age).alias(\"Scaled_Age\"), \n",
    "               sex_to_num_UDF(titanic.Sex).alias(\"Sex_Num\"),\n",
    "               max_int_UDF(titanic.SibSp,titanic.Parch).alias(\"Max_Family\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|  712|\n",
      "+-----+\n",
      "\n",
      "+-------------+\n",
      "|sum(Survived)|\n",
      "+-------------+\n",
      "|          288|\n",
      "+-------------+\n",
      "\n",
      "+------+-------------+\n",
      "|Pclass|sum(Survived)|\n",
      "+------+-------------+\n",
      "|     1|          120|\n",
      "|     3|           85|\n",
      "|     2|           83|\n",
      "+------+-------------+\n",
      "\n",
      "+------+--------+-------------+\n",
      "|Pclass|Embarked|sum(Survived)|\n",
      "+------+--------+-------------+\n",
      "|     3|       C|           18|\n",
      "|     2|       C|            8|\n",
      "|     1|       Q|            1|\n",
      "|     3|       Q|            6|\n",
      "|     2|       Q|            1|\n",
      "|     1|       C|           53|\n",
      "|     1|       S|           66|\n",
      "|     3|       S|           61|\n",
      "|     2|       S|           74|\n",
      "+------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Una única agregación\n",
    "titanic = load_titanic()\n",
    "\n",
    "titanic.groupBy().count().show()\n",
    "titanic.groupBy().sum('Survived').show()\n",
    "titanic.groupBy('Pclass').sum('Survived').show()\n",
    "titanic.groupBy('Pclass','Embarked').sum('Survived').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------------+\n",
      "|Pclass|sum(Survived)|         sum(Fare)|\n",
      "+------+-------------+------------------+\n",
      "|     1|          120| 16200.85429999999|\n",
      "|     3|           85| 4696.449500000006|\n",
      "|     2|           83|3714.5791999999997|\n",
      "+------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Varias agregaciones del mismo tipo\n",
    "titanic = load_titanic()\n",
    "\n",
    "titanic.groupBy('Pclass').sum('Survived', 'Fare').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------+\n",
      "|Pclass|sum(Survived)|count(1)|\n",
      "+------+-------------+--------+\n",
      "|     1|          120|     184|\n",
      "|     3|           85|     355|\n",
      "|     2|           83|     173|\n",
      "+------+-------------+--------+\n",
      "\n",
      "+------+-----+---------+\n",
      "|Pclass|Total|Survivors|\n",
      "+------+-----+---------+\n",
      "|     1|  184|      120|\n",
      "|     3|  355|       85|\n",
      "|     2|  173|       83|\n",
      "+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Varias funciones de agregación diferentes a la vez\n",
    "titanic = load_titanic()\n",
    "\n",
    "# Usando diccionarios\n",
    "titanic.groupBy('Pclass').agg({'*':'count', 'Survived':'sum'}).show()\n",
    "\n",
    "# Usando una secuencia de funciones de la biblioteca pyspark.sql.functions\n",
    "from pyspark.sql import functions\n",
    "titanic.groupBy('Pclass').agg(functions.count('*').alias('Total'), functions.sum('Survived').alias('Survivors')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL sobre DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+----+\n",
      "| id|nombre| id|edad|\n",
      "+---+------+---+----+\n",
      "|  1|   ana|  1|  36|\n",
      "|  2|  jose|  2|  30|\n",
      "+---+------+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ejecución de código SQL que combina DataFrames\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.createOrReplaceTempView(\"users\")\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.createOrReplaceTempView(\"age\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT *\n",
    "             FROM users INNER JOIN age ON users.id == age.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+\n",
      "|Survived|Family|Sex_Num|\n",
      "+--------+------+-------+\n",
      "|       0|     0|      1|\n",
      "|       1|     0|      0|\n",
      "|       1|     0|      0|\n",
      "+--------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic = load_titanic()\n",
    "titanic.createOrReplaceTempView(\"titanic\")\n",
    "spark.sql(\"\"\"SELECT Survived, SibSp+Parch AS Family, sex_to_num(Sex) AS Sex_Num\n",
    "             FROM titanic \n",
    "             WHERE Age > 50\n",
    "             \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+\n",
      "|Pclass|Total|Survivors|\n",
      "+------+-----+---------+\n",
      "|     1|  184|      120|\n",
      "|     2|  173|       83|\n",
      "|     3|  355|       85|\n",
      "+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecución de código SQL con agregación y ordenación\n",
    "titanic = load_titanic()\n",
    "titanic.createOrReplaceTempView(\"titanic\")\n",
    "spark.sql(\"\"\"SELECT Pclass, \n",
    "                    COUNT(*) AS Total, \n",
    "                    SUM(Survived) AS Survivors \n",
    "             FROM titanic \n",
    "             GROUP BY Pclass\n",
    "             ORDER BY Pclass ASC\n",
    "             \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con LinearSVC usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OneHotEncoderEstimator' from 'pyspark.ml.feature' (/home/miguel/Documentos/PROGRAMACION/BIG DATA/env/lib/python3.10/site-packages/pyspark/ml/feature.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIndexer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneHotEncoderEstimator\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorAssembler\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OneHotEncoderEstimator' from 'pyspark.ml.feature' (/home/miguel/Documentos/PROGRAMACION/BIG DATA/env/lib/python3.10/site-packages/pyspark/ml/feature.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "clf = LinearSVC(featuresCol='features', labelCol='Survived')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, clf])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para clasificar 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('prediction', 'Survived')\n",
    "results.show(5)\n",
    "\n",
    "\n",
    "# Evaluación de la clasificación usando la clase experimental MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "claseval = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='Survived', metricName='accuracy')\n",
    "print('Score:', claseval.evaluate(prediction))\n",
    "\n",
    "\n",
    "# Evaluación de la regresión utilizando mllib (obsolescente)\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "rdd = results.rdd.map(lambda row: (row[0], float(row[1]))) # Es necesario representar la clase Survived como float\n",
    "                                                           # o MulticlassMetrics fallará\n",
    "metrics = MulticlassMetrics(rdd)\n",
    "print(\"Score:\", metrics.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con LinearRegression usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "reg = LinearRegression(featuresCol='features', labelCol='Fare')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, reg])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para inferir tarifas a partir de las instancias en 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)\n",
    "\n",
    "# Evaluación utilizando la clase experimental RegressionEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "maeeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='mae')\n",
    "print(\"MAE :\", maeeval.evaluate(results))\n",
    "mseeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='mse')\n",
    "print(\"MSE :\", mseeval.evaluate(results))\n",
    "rmseeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='rmse')\n",
    "print(\"RMSE:\", rmseeval.evaluate(results),'\\n')\n",
    "\n",
    "# Evaluación de la regresión utilizando mllib (obsolescente)\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "rdd = results.rdd\n",
    "metrics = RegressionMetrics(rdd)\n",
    "print(\"MAE :\", metrics.meanAbsoluteError)\n",
    "print(\"MSE :\", metrics.meanSquaredError)\n",
    "print(\"RMSE:\", metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de grupos con k-means usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Descartamos las columnas que no nos interesan y eliminamos filas con valores vacíos\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "# One-hot-encoding de la columna Embarked\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "# Combina todas las columnas en un único vector\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE','Fare'], outputCol='features_raw')\n",
    "# Normaliza los valores de cada posición del vector\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "# Realiza análisis de grupos\n",
    "clu = KMeans(k=3) # valores por defecto: atributos en 'features' y centroide en 'prediction'\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, clu])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline\n",
    "model = pipeline.fit(titanic)\n",
    "prediction = model.transform(titanic)\n",
    "prediction.select('prediction').show(5)\n",
    "\n",
    "# Obtenemos los centroides desde el último modelo del pipiline\n",
    "print('Centroides:')\n",
    "print(type(model.stages[-1].clusterCenters()[0]))\n",
    "for c in model.stages[-1].clusterCenters():\n",
    "    print(c)\n",
    "\n",
    "# Evaluación utilizando la clase experimental ClusteringEvaluator para obtener el coeficiente de silueta\n",
    "evaluator = ClusteringEvaluator()\n",
    "print('Silhouette Coefficient:', evaluator.evaluate(prediction))\n",
    "\n",
    "# No existen métodos de evaluación de clústers en mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia de modelos en Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "reg = LinearRegression(featuresCol='features', labelCol='Fare')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, reg])\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para inferir tarifas a partir de las instancias en 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)\n",
    "\n",
    "# Volcado del modelo\n",
    "model.save('../../data/Cap7/regression_model') # Equivalente a write().save(path)\n",
    "#model.write().overwrite().save('../../data/Cap7/regression_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Carga del modelo\n",
    "loaded_model = PipelineModel.load('../../data/Cap7/regression_model')\n",
    "\n",
    "prediction = loaded_model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
