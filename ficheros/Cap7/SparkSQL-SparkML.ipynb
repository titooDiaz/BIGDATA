{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fdikike.fdi.ucm.es:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa5dc247588>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  b|  9.4|\n",
      "|  a|  2.7|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('a',3.14), ('b', 9.4), ('a',2.7)]\n",
    "headers = ['id','value']\n",
    "df = spark.createDataFrame(l, headers)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('a',3.14), ('b',True)]\n",
    "headers = ['id','value']\n",
    "## df = spark.createDataFrame(l, headers) \n",
    "## Esta invocación lanzaría una excepción \n",
    "## TypeError: field value: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.BooleanType'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "l = [('a',3.14), ('b', 9.4), (None, 5.7)]\n",
    "schema = StructType([\n",
    "  StructField('id',   StringType(), True),\n",
    "  StructField('value', FloatType(), False)\n",
    "])\n",
    "df = spark.createDataFrame(l, schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "l = [('a',3.14), ('b', 9.4), ('a', True)]\n",
    "schema = StructType([\n",
    "  StructField('id',   StringType(), True),\n",
    "  StructField('value', FloatType(), False)\n",
    "])\n",
    "## df = spark.createDataFrame(l, schema) ## Lanza excepción\n",
    "## TypeError: field value: FloatType can not accept object True in type <class 'bool'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  b|  9.4|\n",
      "|  a|  2.7|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = sc.parallelize([('a',3.14), ('b', 9.4), ('a', 2.7)])\n",
    "df = spark.createDataFrame(r, ['id','value'])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "| _1| _2| _3|                  _4|    _5| _6| _7| _8|              _9|    _10|_11|_12|\n",
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "|  1|  0|  3|Braund, Mr. Owen ...|  male| 22|  1|  0|       A/5 21171|   7.25|   |  S|\n",
      "|  2|  1|  1|Cumings, Mrs. Joh...|female| 38|  1|  0|        PC 17599|71.2833|C85|  C|\n",
      "|  3|  1|  3|Heikkinen, Miss. ...|female| 26|  0|  0|STON/O2. 3101282|  7.925|   |  S|\n",
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925|     |       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creación de un DataFrame a partir de un RDD\n",
    "import csv\n",
    "\n",
    "raw = (\n",
    "  sc.textFile(\"../../data/Cap7/titanic.csv\") # Leemos el fichero de texto\n",
    "    .map(lambda s: list(csv.reader([s]))[0]) # Dividimos el CSV en listas\n",
    "    .filter(lambda l: l[0] != 'PassengerId') # Eliminamos la cabecera\n",
    ")\n",
    "\n",
    "df = spark.createDataFrame(raw)\n",
    "df.show(3)\n",
    "df.printSchema()\n",
    "headers = ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "df = spark.createDataFrame(raw, headers)\n",
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y escritura de DataFrames desde ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RT_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n",
      "+--------+--------------+----------------+\n",
      "|RT_count|          text|            user|\n",
      "+--------+--------------+----------------+\n",
      "|       2|   #Tengosueño| [3, Pepe, true]|\n",
      "|      45|  #VivaElLunes|[15, Ana, false]|\n",
      "|     100|¡Gol de Señor!|  [2, Eva, true]|\n",
      "+--------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../../data/Cap7/tweets.json')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|En el anterior ap...|\n",
      "|y que se crean me...|\n",
      "|antes de poder co...|\n",
      "|un RDD (o varios)...|\n",
      "|Estos RDDs se cre...|\n",
      "|o a partir de fic...|\n",
      "|creará un RDD y s...|\n",
      "|diferentes proces...|\n",
      "|En el anterior ap...|\n",
      "|y que se crean me...|\n",
      "|antes de poder co...|\n",
      "|un RDD (o varios)...|\n",
      "|Estos RDDs se cre...|\n",
      "|o a partir de fic...|\n",
      "|creará un RDD y s...|\n",
      "|diferentes proces...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lee todos los ficheros, y cada línea es una fila del DataFrame.\n",
    "## El DataFrame tiene una única columna 'value' de tipo string\n",
    "df = spark.read.text('../../data/Cap7/*.txt')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|En el anterior ap...|\n",
      "|En el anterior ap...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lee todos los ficheros, y cada ficheros es una fila del DataFrame\n",
    "## El DataFrame tiene una única columna 'value' de tipo string\n",
    "df = spark.read.text('../../data/Cap7/*.txt', wholetext=True)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escritura de un DataFrame de 50 elementos en la carpeta /tmp/csv\n",
    "l = [('a',3.14)] * 50\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.printSchema()\n",
    "df.write.csv('/tmp/csv', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del DataFrame anterior desde la carpeta /tmp/csv\n",
    "df = spark.read.csv('/tmp/csv', header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(4)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "l = [('a',3.14)] * 50\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.printSchema()\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.json('/tmp/json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del DataFrame anterior desde la carpeta /tmp/json\n",
    "df = spark.read.json('/tmp/json')\n",
    "df.printSchema()\n",
    "df.show(4)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  value\n",
       "0   a   3.14\n",
       "1   a   3.14\n",
       "2   a   3.14\n",
       "3   a   3.14\n",
       "4   a   3.14\n",
       "5   a   3.14\n",
       "6   a   3.14\n",
       "7   a   3.14\n",
       "8   a   3.14\n",
       "9   a   3.14\n",
       "10  a   3.14\n",
       "11  a   3.14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Volcar un DataFrame a Pandas\n",
    "l = [('a',3.14)] * 12\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames y MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder cargar y salvar DataFrames en MongoDB es necesario lanzar ```pyspark``` configurando el conector de Mongo-Spark. Para ello es necesario pasar el siguiente parámetro a la hora de invocar a ```pyspark```:\n",
    "```\n",
    "pyspark --packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.2 --master local[*]\n",
    "```\n",
    "        \n",
    "El nombre del conector corresponde a _coordenadas_ Maven:\n",
    " * **2.11** porque el conector utiliza la versión 2.11 de Scala\n",
    " * **2.2.2** es la versión concreta del conector. La versión 2.2.2 soporta MongoDB 2.2.x y 2.3.x\n",
    " \n",
    "El parámetro ```--master``` es el usual. En este caso lanzamos ```pyspark``` en modo local y utilizando tantos procesos _workers_ como núcleos tenga nuestra CPU.\n",
    "\n",
    "Se puede encontrar más información en:\n",
    " * https://docs.mongodb.com/spark-connector/master/\n",
    " * https://docs.mongodb.com/spark-connector/master/python-api/\n",
    " \n",
    "**Supondremos que existe un servidor MongoDB ejecutándose en local (IP 127.0.0.1) y escuchando en el puerto por defecto (27017).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|   nombre|habitantes|\n",
      "+---------+----------+\n",
      "|   Madrid|   3182981|\n",
      "|Barcelona|   1620809|\n",
      "| Valencia|    787808|\n",
      "|  Sevilla|    689434|\n",
      "| Zaragoza|    664938|\n",
      "|   Málaga|    569002|\n",
      "|   Murcia|    443243|\n",
      "|    Palma|    406492|\n",
      "+---------+----------+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- habitantes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Volcar un DataFrame a una colección MongoDB\n",
    "ciudades = spark.createDataFrame([(\"Madrid\",  3182981), (\"Barcelona\", 1620809), (\"Valencia\",787808),\n",
    "                                  (\"Sevilla\", 689434), (\"Zaragoza\", 664938), (\"Málaga\",569002), \n",
    "                                  (\"Murcia\",443243), (\"Palma\",406492) ], [\"nombre\", \"habitantes\"])\n",
    "ciudades.show()\n",
    "ciudades.printSchema()\n",
    "\n",
    "(ciudades.write.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "              .option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\").save()\n",
    ")\n",
    "# *Importante*: La colección test.ciudades no debe existir. Si lo que queremos es añadir a una colección ya existente, debemos usar mode(\"append\"):\n",
    "### ciudades.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|                 _id|habitantes|   nombre|\n",
      "+--------------------+----------+---------+\n",
      "|[5baa6d4ac68b5f61...|   1620809|Barcelona|\n",
      "|[5baa6d4ac68b5f61...|    664938| Zaragoza|\n",
      "|[5baa6d4ac68b5f61...|    787808| Valencia|\n",
      "|[5baa6d4ac68b5f61...|    689434|  Sevilla|\n",
      "|[5baa6d4ac68b5f61...|    569002|   Málaga|\n",
      "|[5baa6d4ac68b5f61...|    443243|   Murcia|\n",
      "|[5baa6d4ac68b5f61...|    406492|    Palma|\n",
      "|[5baa6d4ac68b5f61...|   3182981|   Madrid|\n",
      "+--------------------+----------+---------+\n",
      "\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- habitantes: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar un DataFrame desde una colección MongoDB\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\").load()\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----+------+\n",
      "|                 _id|altura|edad|nombre|\n",
      "+--------------------+------+----+------+\n",
      "|[5baa6d4a09fcae61...|  null|  33|   ana|\n",
      "|[5baa6d4a09fcae61...|   150|null| pedro|\n",
      "+--------------------+------+----+------+\n",
      "\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- altura: integer (nullable = true)\n",
      " |-- edad: integer (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar un DataFrame desde una colección MongoDB heterogénea (distintos documentos tienen atributos diferentes)\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Borramos la colección test.usuarios e insertamos dos documentos heterogéneos\n",
    "client = MongoClient('127.0.0.1')\n",
    "col = client['test']['usuarios']\n",
    "col.drop()\n",
    "col.insert_many([{'nombre':'ana', 'edad':33}, {'nombre':'pedro','altura':150}])\n",
    "\n",
    "# Cargamos un DataFrame a partir de la colección test.usuarios\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.usuarios\").load()\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|                 _id|habitantes|   nombre|\n",
      "+--------------------+----------+---------+\n",
      "|[5baa6d4ac68b5f61...|   1620809|Barcelona|\n",
      "|[5baa6d4ac68b5f61...|    664938| Zaragoza|\n",
      "|[5baa6d4ac68b5f61...|    787808| Valencia|\n",
      "|[5baa6d4ac68b5f61...|    689434|  Sevilla|\n",
      "|[5baa6d4ac68b5f61...|    569002|   Málaga|\n",
      "|[5baa6d4ac68b5f61...|   3182981|   Madrid|\n",
      "+--------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = \"{'$match': {'habitantes': {$gt:500000}}}\"\n",
    "masde500mil = (spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "                        .option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\")\n",
    "                        .option(\"pipeline\", pipeline).load()\n",
    "              )\n",
    "masde500mil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+\n",
      "|summary|  id|value|\n",
      "+-------+----+-----+\n",
      "|  count|  12|   12|\n",
      "|   mean|null| 3.14|\n",
      "| stddev|null|  0.0|\n",
      "|    min|   a| 3.14|\n",
      "|    max|   a| 3.14|\n",
      "+-------+----+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "l = [('a',3.14)] * 12\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.describe().show()\n",
    "type(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [('a',3.14), ('b',2.0), ('c',4.5)]\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------+-----+\n",
      "|summary|               Age|             Fare|   Sex|Cabin|\n",
      "+-------+------------------+-----------------+------+-----+\n",
      "|  count|               714|              891|   891|  204|\n",
      "|   mean| 29.69911764705882| 32.2042079685746|  null| null|\n",
      "| stddev|14.526497332334035|49.69342859718089|  null| null|\n",
      "|    min|              0.42|              0.0|female|  A10|\n",
      "|    max|              80.0|         512.3292|  male|    T|\n",
      "+-------+------------------+-----------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "df.describe(['Age','Fare','Sex','Cabin']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']\n",
      "['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "## Eliminación de columnas con drop()\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.columns)\n",
    "df2 = titanic.drop('PassengerId','Name','Cabin')\n",
    "print(df2.columns)\n",
    "df3 = titanic.drop(titanic.PassengerId)\n",
    "print(df3.columns)\n",
    "df4 = titanic.drop(titanic['PassengerId'])\n",
    "print(df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "['Survived', 'Pclass', 'Age']\n"
     ]
    }
   ],
   "source": [
    "## Selección de columnas con select()\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.columns)\n",
    "df2 = titanic.select('Survived', 'Pclass', 'Age')\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "891\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.count())\n",
    "df = titanic.dropDuplicates()\n",
    "print(df.count())\n",
    "df = titanic.dropDuplicates(['Sex'])\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n",
      "183\n",
      "712\n"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.count())\n",
    "df = titanic.dropna()\n",
    "print(df.count())\n",
    "df = titanic.drop('Cabin').dropna()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n",
      "342\n"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "df = titanic.filter( 'Survived = 1')\n",
    "print(df.count())\n",
    "df = titanic.filter( df.Survived == 1)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "df = titanic.filter( 'Survived = 1 AND Sex = \"female\" AND Age > 20')\n",
    "print(df.count())\n",
    "df = titanic.filter( (df.Survived == 1) & (df['Sex'] == 'female') & (df.Age > 20))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Filas cuyo camarote tiene 3 caracteres, el primero es una A o una B mayúsculas y el último un 2\n",
    "# Usa una expresión regular\n",
    "df = titanic.filter('Cabin RLIKE \"^[AB].2$\"')\n",
    "print(df.count())\n",
    "df = titanic.filter(df.Cabin.rlike('^[AB].2$'))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "|  3| marta|\n",
      "|  1|   ana|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.union(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|1.0|   ana|\n",
      "|2.0|  jose|\n",
      "|3.0| marta|\n",
      "|1.0|   ana|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(3.0,'marta'),(1.0,'ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "df = df1.union(df2) ## No hay problema: long y double son compatibles => double\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "|  3| marta|\n",
      "|  1|   ana|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([('3','marta'),('1','ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "df = df1.union(df2) ## No hay problema: long y string son compatibles => string\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: boolean (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(True,'marta'),(False,'ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "## df = df1.union(df2) ## Excepcion\n",
    "##AnalysisException: \"Union can only be performed on tables with the compatible column types. boolean <> bigint at the first column of the second table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(3,'marta',33),(4,'señor',44)],['id','nombre','edad'])\n",
    "df2.printSchema()\n",
    "## df = df1.union(df2)  ## Excepción\n",
    "## AnalysisException: \"Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.intersect(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  2|  jose|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.subtract(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "+---+------+\n",
      "\n",
      "+---+----+\n",
      "| id|edad|\n",
      "+---+----+\n",
      "|  1|  36|\n",
      "|  2|  30|\n",
      "+---+----+\n",
      "\n",
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   ana|  36|\n",
      "|  2|  jose|  30|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## inner join de dos DataFrames sobre la columna 'id'\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.show()\n",
    "df = users.join(age,'id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- edad: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- edad: long (nullable = true)\n",
      "\n",
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   ana|  36|\n",
      "|  2|  jose|  30|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Inner join donde la columna 'id' contiene enteros y cadenas de texto\n",
    "users = spark.createDataFrame([('1','ana'),('2','jose')],['id','nombre'])\n",
    "users.printSchema()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.printSchema()\n",
    "df = users.join(age,'id') ## No hay problema: string y long son compatibles => string\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "| id|nombre|deporte|\n",
      "+---+------+-------+\n",
      "|  1|   ana|   golf|\n",
      "|  2|  jose|   polo|\n",
      "+---+------+-------+\n",
      "\n",
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   eva|  33|\n",
      "|  2|  jose|  30|\n",
      "+---+------+----+\n",
      "\n",
      "+---+------+-------+----+\n",
      "| id|nombre|deporte|edad|\n",
      "+---+------+-------+----+\n",
      "|  2|  jose|   polo|  30|\n",
      "+---+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## inner join considerando igualdad de varias columnas a la vez\n",
    "users = spark.createDataFrame([(1,'ana','golf'),(2,'jose','polo',)],['id','nombre','deporte'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,'eva',33),(2,'jose',30)],['id','nombre','edad'])\n",
    "age.show()\n",
    "df = users.join(age,[\"id\", \"nombre\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "+---+------+\n",
      "\n",
      "+-----+----+\n",
      "|ident|edad|\n",
      "+-----+----+\n",
      "|    1|  36|\n",
      "|    2|  30|\n",
      "+-----+----+\n",
      "\n",
      "+---+------+-----+----+\n",
      "| id|nombre|ident|edad|\n",
      "+---+------+-----+----+\n",
      "|  1|   ana|    1|  36|\n",
      "|  2|  jose|    2|  30|\n",
      "+---+------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## inner join usando una expresión de igualdad entre 2 columnas\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['ident','edad'])\n",
    "age.show()\n",
    "df = users.join(age,users.id == age.ident)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|sport|\n",
      "+---+----+-----+\n",
      "|  1| ana| golf|\n",
      "|  2|jose| polo|\n",
      "+---+----+-----+\n",
      "\n",
      "+-----+------+----+\n",
      "|ident|nombre|edad|\n",
      "+-----+------+----+\n",
      "|    1|   eva|  33|\n",
      "|    2|  jose|  30|\n",
      "+-----+------+----+\n",
      "\n",
      "+---+----+-----+-----+------+----+\n",
      "| id|name|sport|ident|nombre|edad|\n",
      "+---+----+-----+-----+------+----+\n",
      "|  2|jose| polo|    2|  jose|  30|\n",
      "+---+----+-----+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## inner join usando una expresión de igualdad entre 4 columnas\n",
    "users = spark.createDataFrame([(1,'ana','golf'),(2,'jose','polo',)],['id','name','sport'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,'eva',33),(2,'jose',30)],['ident','nombre','edad'])\n",
    "age.show()\n",
    "df = users.join(age,(users.id == age.ident) & (users.name == age.nombre))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "+---+------+\n",
      "\n",
      "+----+---+----+\n",
      "| id1|id2|edad|\n",
      "+----+---+----+\n",
      "|null|  1|  33|\n",
      "|   2|  5|  30|\n",
      "+----+---+----+\n",
      "\n",
      "+---+------+----+---+----+\n",
      "| id|nombre| id1|id2|edad|\n",
      "+---+------+----+---+----+\n",
      "|  1|   ana|null|  1|  33|\n",
      "|  2|  jose|   2|  5|  30|\n",
      "+---+------+----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## inner join usando una expresión compleja entre 2 columnas\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(None,1,33),(2,5,30)],['id1','id2','edad'])\n",
    "age.show()\n",
    "cond = ((age.id1.isNotNull() & (users.id == age.id1)) | \n",
    "       (age.id1.isNull() & (users.id == age.id2)))\n",
    "df = users.join(age,cond)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|nombre|\n",
      "+---+------+\n",
      "|  1|   ana|\n",
      "|  2|  jose|\n",
      "|  3|   eva|\n",
      "+---+------+\n",
      "\n",
      "+---+----+\n",
      "| id|edad|\n",
      "+---+----+\n",
      "|  1|  36|\n",
      "|  2|  30|\n",
      "+---+----+\n",
      "\n",
      "+---+------+----+\n",
      "| id|nombre|edad|\n",
      "+---+------+----+\n",
      "|  1|   ana|  36|\n",
      "|  3|   eva|null|\n",
      "|  2|  jose|  30|\n",
      "+---+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## left outer join de users y age\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose'),(3,'eva')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.show()\n",
    "df = users.join(age,'id','left_outer')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_titanic():\n",
    "    return spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True).drop('Cabin').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+\n",
      "|Survived|Family|  Age|\n",
      "+--------+------+-----+\n",
      "|       0|     1|264.0|\n",
      "|       1|     1|456.0|\n",
      "|       1|     0|312.0|\n",
      "+--------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operaciones aritméticas entre columnas\n",
    "titanic = load_titanic()\n",
    "titanic.selectExpr(\"Survived\",\"SibSp + Parch As Family\", \"Age * 12 AS Age\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|   Sex|Sex_num|\n",
      "+------+-------+\n",
      "|  male|      1|\n",
      "|female|      0|\n",
      "|female|      0|\n",
      "+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "def sex_to_num(s):\n",
    "    ret = None\n",
    "    if s == 'female':\n",
    "        ret = 0\n",
    "    elif s == 'male':\n",
    "        ret = 1\n",
    "    return ret\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark.udf.register(\"sex_to_num\", sex_to_num, IntegerType())\n",
    "\n",
    "titanic.selectExpr(\"Sex\", \"sex_to_num(Sex) AS Sex_num\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+\n",
      "|SibSp|Parch|Max_Family|\n",
      "+-----+-----+----------+\n",
      "|    1|    0|         1|\n",
      "|    1|    0|         1|\n",
      "|    0|    0|         0|\n",
      "+-----+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "spark.udf.register(\"max_int\", max, IntegerType())\n",
    "    \n",
    "titanic.selectExpr(\"SibSp\", \"Parch\", \"max_int(SibSp, Parch) AS Max_Family\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "| Age|         Scaled_Age|\n",
      "+----+-------------------+\n",
      "|22.0| 0.2711736617240513|\n",
      "|38.0| 0.4722292033174164|\n",
      "|26.0|0.32143754712239253|\n",
      "+----+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "def scale(n,minv,maxv):\n",
    "    return (n - minv) / (maxv - minv)\n",
    "\n",
    "# Se puede obtener los valores minimos y maximos de cada columna a través del DataFrame generado por describe()\n",
    "summary = titanic.describe().toPandas()\n",
    "min_age = float(summary.loc[3,'Age'])\n",
    "max_age = float(summary.loc[4,'Age'])\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "spark.udf.register(\"scale_Age\", lambda x: scale(x, min_age, max_age), DoubleType())\n",
    "\n",
    "titanic.selectExpr(\"Age\", \"scale_Age(Age) AS Scaled_Age\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+\n",
      "|Survived|Family|  Age|\n",
      "+--------+------+-----+\n",
      "|       0|     1|264.0|\n",
      "|       1|     1|456.0|\n",
      "|       1|     0|312.0|\n",
      "+--------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformación de columnas usando expresiones entre columnas\n",
    "titanic = load_titanic()\n",
    "titanic.select(titanic.Survived,(titanic.SibSp + titanic.Parch).alias(\"Family\"), (titanic.Age * 12).alias(\"Age\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----------+\n",
      "|         Scaled_Age|Sex_Num|Max_Family|\n",
      "+-------------------+-------+----------+\n",
      "| 0.2711736617240513|      1|         1|\n",
      "| 0.4722292033174164|      0|         1|\n",
      "|0.32143754712239253|      0|         0|\n",
      "+-------------------+-------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformaciones con UDFs usando expresiones entre columnas\n",
    "from pyspark.sql.functions import udf\n",
    "titanic = load_titanic()\n",
    "\n",
    "sex_to_num_UDF = udf(sex_to_num, IntegerType())\n",
    "\n",
    "max_int_UDF = udf(max, IntegerType())\n",
    "\n",
    "summary = titanic.describe().toPandas()\n",
    "min_age = float(summary.loc[3,'Age'])\n",
    "max_age = float(summary.loc[4,'Age'])\n",
    "scale_Age_UDF = udf(lambda x : scale(x, min_age, max_age), DoubleType())\n",
    "                   \n",
    "titanic.select(scale_Age_UDF(titanic.Age).alias(\"Scaled_Age\"), \n",
    "               sex_to_num_UDF(titanic.Sex).alias(\"Sex_Num\"),\n",
    "               max_int_UDF(titanic.SibSp,titanic.Parch).alias(\"Max_Family\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|  712|\n",
      "+-----+\n",
      "\n",
      "+-------------+\n",
      "|sum(Survived)|\n",
      "+-------------+\n",
      "|          288|\n",
      "+-------------+\n",
      "\n",
      "+------+-------------+\n",
      "|Pclass|sum(Survived)|\n",
      "+------+-------------+\n",
      "|     1|          120|\n",
      "|     3|           85|\n",
      "|     2|           83|\n",
      "+------+-------------+\n",
      "\n",
      "+------+--------+-------------+\n",
      "|Pclass|Embarked|sum(Survived)|\n",
      "+------+--------+-------------+\n",
      "|     3|       C|           18|\n",
      "|     2|       C|            8|\n",
      "|     1|       Q|            1|\n",
      "|     3|       Q|            6|\n",
      "|     2|       Q|            1|\n",
      "|     1|       C|           53|\n",
      "|     1|       S|           66|\n",
      "|     3|       S|           61|\n",
      "|     2|       S|           74|\n",
      "+------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Una única agregación\n",
    "titanic = load_titanic()\n",
    "\n",
    "titanic.groupBy().count().show()\n",
    "titanic.groupBy().sum('Survived').show()\n",
    "titanic.groupBy('Pclass').sum('Survived').show()\n",
    "titanic.groupBy('Pclass','Embarked').sum('Survived').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------------+\n",
      "|Pclass|sum(Survived)|         sum(Fare)|\n",
      "+------+-------------+------------------+\n",
      "|     1|          120| 16200.85429999999|\n",
      "|     3|           85| 4696.449500000006|\n",
      "|     2|           83|3714.5791999999997|\n",
      "+------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Varias agregaciones del mismo tipo\n",
    "titanic = load_titanic()\n",
    "\n",
    "titanic.groupBy('Pclass').sum('Survived', 'Fare').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------+\n",
      "|Pclass|sum(Survived)|count(1)|\n",
      "+------+-------------+--------+\n",
      "|     1|          120|     184|\n",
      "|     3|           85|     355|\n",
      "|     2|           83|     173|\n",
      "+------+-------------+--------+\n",
      "\n",
      "+------+-----+---------+\n",
      "|Pclass|Total|Survivors|\n",
      "+------+-----+---------+\n",
      "|     1|  184|      120|\n",
      "|     3|  355|       85|\n",
      "|     2|  173|       83|\n",
      "+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Varias funciones de agregación diferentes a la vez\n",
    "titanic = load_titanic()\n",
    "\n",
    "# Usando diccionarios\n",
    "titanic.groupBy('Pclass').agg({'*':'count', 'Survived':'sum'}).show()\n",
    "\n",
    "# Usando una secuencia de funciones de la biblioteca pyspark.sql.functions\n",
    "from pyspark.sql import functions\n",
    "titanic.groupBy('Pclass').agg(functions.count('*').alias('Total'), functions.sum('Survived').alias('Survivors')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL sobre DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+----+\n",
      "| id|nombre| id|edad|\n",
      "+---+------+---+----+\n",
      "|  1|   ana|  1|  36|\n",
      "|  2|  jose|  2|  30|\n",
      "+---+------+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecución de código SQL que combina DataFrames\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.createOrReplaceTempView(\"users\")\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.createOrReplaceTempView(\"age\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT *\n",
    "             FROM users INNER JOIN age ON users.id == age.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+\n",
      "|Survived|Family|Sex_Num|\n",
      "+--------+------+-------+\n",
      "|       0|     0|      1|\n",
      "|       1|     0|      0|\n",
      "|       1|     0|      0|\n",
      "+--------+------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic = load_titanic()\n",
    "titanic.createOrReplaceTempView(\"titanic\")\n",
    "spark.sql(\"\"\"SELECT Survived, SibSp+Parch AS Family, sex_to_num(Sex) AS Sex_Num\n",
    "             FROM titanic \n",
    "             WHERE Age > 50\n",
    "             \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+\n",
      "|Pclass|Total|Survivors|\n",
      "+------+-----+---------+\n",
      "|     1|  184|      120|\n",
      "|     2|  173|       83|\n",
      "|     3|  355|       85|\n",
      "+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecución de código SQL con agregación y ordenación\n",
    "titanic = load_titanic()\n",
    "titanic.createOrReplaceTempView(\"titanic\")\n",
    "spark.sql(\"\"\"SELECT Pclass, \n",
    "                    COUNT(*) AS Total, \n",
    "                    SUM(Survived) AS Survivors \n",
    "             FROM titanic \n",
    "             GROUP BY Pclass\n",
    "             ORDER BY Pclass ASC\n",
    "             \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con LinearSVC usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|prediction|Survived|\n",
      "+----------+--------+\n",
      "|       0.0|       0|\n",
      "|       0.0|       0|\n",
      "|       0.0|       0|\n",
      "|       0.0|       0|\n",
      "|       0.0|       0|\n",
      "+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Score: 0.7480916030534351\n",
      "Score: 0.7480916030534351\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "clf = LinearSVC(featuresCol='features', labelCol='Survived')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, clf])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para clasificar 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('prediction', 'Survived')\n",
    "results.show(5)\n",
    "\n",
    "\n",
    "# Evaluación de la clasificación usando la clase experimental MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "claseval = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='Survived', metricName='accuracy')\n",
    "print('Score:', claseval.evaluate(prediction))\n",
    "\n",
    "\n",
    "# Evaluación de la regresión utilizando mllib (obsolescente)\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "rdd = results.rdd.map(lambda row: (row[0], float(row[1]))) # Es necesario representar la clase Survived como float\n",
    "                                                           # o MulticlassMetrics fallará\n",
    "metrics = MulticlassMetrics(rdd)\n",
    "print(\"Score:\", metrics.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con LinearRegression usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+\n",
      "|        Prediction|    Fare|\n",
      "+------------------+--------+\n",
      "| 77.19787404192037| 77.2875|\n",
      "| 62.41948632392477|     5.0|\n",
      "|60.089804041297455|    52.0|\n",
      "|  99.9158323353225|110.8833|\n",
      "| 94.36434230867826| 61.3792|\n",
      "+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "MAE : 24.07650925132509\n",
      "MSE : 2166.7990121545654\n",
      "RMSE: 46.54888840944073 \n",
      "\n",
      "MAE : 24.07650925132509\n",
      "MSE : 2166.7990121545654\n",
      "RMSE: 46.54888840944073\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "reg = LinearRegression(featuresCol='features', labelCol='Fare')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, reg])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para inferir tarifas a partir de las instancias en 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)\n",
    "\n",
    "# Evaluación utilizando la clase experimental RegressionEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "maeeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='mae')\n",
    "print(\"MAE :\", maeeval.evaluate(results))\n",
    "mseeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='mse')\n",
    "print(\"MSE :\", mseeval.evaluate(results))\n",
    "rmseeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='rmse')\n",
    "print(\"RMSE:\", rmseeval.evaluate(results),'\\n')\n",
    "\n",
    "# Evaluación de la regresión utilizando mllib (obsolescente)\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "rdd = results.rdd\n",
    "metrics = RegressionMetrics(rdd)\n",
    "print(\"MAE :\", metrics.meanAbsoluteError)\n",
    "print(\"MSE :\", metrics.meanSquaredError)\n",
    "print(\"RMSE:\", metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de grupos con k-means usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         0|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Centroides:\n",
      "<class 'numpy.ndarray'>\n",
      "[0.7575406  0.87006961 0.37345076 0.09791183 0.05800464 0.06960557\n",
      " 0.11600928 0.         0.04299367]\n",
      "[0.92592593 0.59259259 0.34540597 0.14814815 0.06790123 0.25925926\n",
      " 0.         1.         0.03046664]\n",
      "[0.35433071 0.24409449 0.35892125 0.10629921 0.09645669 0.98818898\n",
      " 0.31496063 0.00393701 0.11293829]\n",
      "Silhouette Coefficient: 0.5096854350632269\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Descartamos las columnas que no nos interesan y eliminamos filas con valores vacíos\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "# One-hot-encoding de la columna Embarked\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "# Combina todas las columnas en un único vector\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE','Fare'], outputCol='features_raw')\n",
    "# Normaliza los valores de cada posición del vector\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "# Realiza análisis de grupos\n",
    "clu = KMeans(k=3) # valores por defecto: atributos en 'features' y centroide en 'prediction'\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, clu])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline\n",
    "model = pipeline.fit(titanic)\n",
    "prediction = model.transform(titanic)\n",
    "prediction.select('prediction').show(5)\n",
    "\n",
    "# Obtenemos los centroides desde el último modelo del pipiline\n",
    "print('Centroides:')\n",
    "print(type(model.stages[-1].clusterCenters()[0]))\n",
    "for c in model.stages[-1].clusterCenters():\n",
    "    print(c)\n",
    "\n",
    "# Evaluación utilizando la clase experimental ClusteringEvaluator para obtener el coeficiente de silueta\n",
    "evaluator = ClusteringEvaluator()\n",
    "print('Silhouette Coefficient:', evaluator.evaluate(prediction))\n",
    "\n",
    "# No existen métodos de evaluación de clústers en mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia de modelos en Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|       Prediction|    Fare|\n",
      "+-----------------+--------+\n",
      "|94.93821195351214|247.5208|\n",
      "|63.96965715934703|    47.1|\n",
      "| 63.5952441835162| 50.4958|\n",
      "| 69.5737043575346|    52.0|\n",
      "|68.82487840587294|    53.1|\n",
      "+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "reg = LinearRegression(featuresCol='features', labelCol='Fare')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, reg])\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para inferir tarifas a partir de las instancias en 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)\n",
    "\n",
    "# Volcado del modelo\n",
    "model.save('../../data/Cap7/regression_model') # Equivalente a write().save(path)\n",
    "#model.write().overwrite().save('../../data/Cap7/regression_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|       Prediction|    Fare|\n",
      "+-----------------+--------+\n",
      "|94.93821195351214|247.5208|\n",
      "|63.96965715934703|    47.1|\n",
      "| 63.5952441835162| 50.4958|\n",
      "| 69.5737043575346|    52.0|\n",
      "|68.82487840587294|    53.1|\n",
      "+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Carga del modelo\n",
    "loaded_model = PipelineModel.load('../../data/Cap7/regression_model')\n",
    "\n",
    "prediction = loaded_model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
