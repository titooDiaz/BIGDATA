{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/12 08:51:16 WARN Utils: Your hostname, miguel-9051-900-0098 resolves to a loopback address: 127.0.1.1; using 192.168.1.58 instead (on interface enp6s0)\n",
      "24/05/12 08:51:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/12 08:51:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crea una sesión Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MiApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Ahora spark es tu SparkSession y no necesitas crear un SparkContext explícitamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  b|  9.4|\n",
      "|  a|  2.7|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('a',3.14), ('b', 9.4), ('a',2.7)]\n",
    "headers = ['id','value']\n",
    "df = spark.createDataFrame(l, headers)\n",
    "n = 3 #\n",
    "df.show(n) # <-- Nos mostrara los primero n elementos del dataframe (por defecto son 20)\n",
    "\n",
    "df.printSchema() # <-- Spark infiere un esquema de datos, que podemos verlo asi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('a',3.14), ('b',True)]\n",
    "headers = ['id','value']\n",
    "## df = spark.createDataFrame(l, headers) \n",
    "## Esta invocación lanzaría una excepción \n",
    "## TypeError: field value: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.BooleanType'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si no le pasamos los parametros del header el los rellenara automaticamente con valores consecutivos _1, _2, _3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  a|  3|\n",
      "+---+---+\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = [('a',1), ('b', 2), ('a',3)]\n",
    "df = spark.createDataFrame(l)\n",
    "df.show(n)\n",
    "\n",
    "df.printSchema() # <-- Spark sigue infiriendo el esquema de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: float (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "l = [('a',3.14), ('b', 9.4), (None, 5.7)]\n",
    "# VAMOS A ASIGNARUN ESQUEMA EN UN DATAFRAME, PARA ESO DEBEMOS CREAR UN OBJETO DE TIPO ** StrucType **\n",
    "# Que contenga tantos objetos StructField como columnas vaya a temner nuestro DataFrame\n",
    "schema = StructType([\n",
    "  #           ESTE ACEPTA 3 PARAMATROS\n",
    "  #           Columna, TIpo de dato, admite valores booleanos?\n",
    "  StructField('id',   StringType(), True),\n",
    "  StructField('value', FloatType(), False)\n",
    "])\n",
    "df = spark.createDataFrame(l, schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "l = [('a',3.14), ('b', 9.4), ('a', True)]\n",
    "schema = StructType([\n",
    "  StructField('id',   StringType(), True),\n",
    "  StructField('value', FloatType(), False)\n",
    "])\n",
    "## df = spark.createDataFrame(l, schema) ## Lanza excepción\n",
    "## TypeError: field value: FloatType can not accept object True in type <class 'bool'>\n",
    "\n",
    "# Error porque solo peude contener valoresde tipo flotante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spak context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  b|  9.4|\n",
      "|  a|  2.7|\n",
      "+---+-----+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = sc.parallelize([('a',3.14), ('b', 9.4), ('a', 2.7)])\n",
    "df = spark.createDataFrame(r, ['id','value'])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Creacion de un DataFrame a partir de un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "| _1| _2| _3|                  _4|    _5| _6| _7| _8|              _9|    _10|_11|_12|\n",
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "|  1|  0|  3|Braund, Mr. Owen ...|  male| 22|  1|  0|       A/5 21171|   7.25|   |  S|\n",
      "|  2|  1|  1|Cumings, Mrs. Joh...|female| 38|  1|  0|        PC 17599|71.2833|C85|  C|\n",
      "|  3|  1|  3|Heikkinen, Miss. ...|female| 26|  0|  0|STON/O2. 3101282|  7.925|   |  S|\n",
      "+---+---+---+--------------------+------+---+---+---+----------------+-------+---+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: string (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      " |-- _7: string (nullable = true)\n",
      " |-- _8: string (nullable = true)\n",
      " |-- _9: string (nullable = true)\n",
      " |-- _10: string (nullable = true)\n",
      " |-- _11: string (nullable = true)\n",
      " |-- _12: string (nullable = true)\n",
      "\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25|     |       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925|     |       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creación de un DataFrame a partir de un RDD\n",
    "import csv\n",
    "\n",
    "raw = (\n",
    "  sc.textFile(\"../../data/Cap7/titanic.csv\") # Leemos el fichero de texto\n",
    "    .map(lambda s: list(csv.reader([s]))[0]) # Dividimos el CSV en listas\n",
    "    .filter(lambda l: l[0] != 'PassengerId') # Eliminamos la cabecera, entonces por defecto quedan numeros...\n",
    ") # <---- ESTE ES EL RDD\n",
    "\n",
    "df = spark.createDataFrame(raw)\n",
    "df.show(3)\n",
    "df.printSchema()\n",
    "headers = ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\n",
    "df = spark.createDataFrame(raw, headers)\n",
    "df.show(3)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y escritura de DataFrames desde ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque no le pasemos parametros en json printSchema tambein peude recibr parametros como csv!\n",
    "Aunque no hay problema si queda vacia, ya que sprk intentaq predecirlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RT_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- user: struct (nullable = true)\n",
      " |    |-- followers_count: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- verified: boolean (nullable = true)\n",
      "\n",
      "+--------+--------------+----------------+\n",
      "|RT_count|          text|            user|\n",
      "+--------+--------------+----------------+\n",
      "|       2|   #Tengosueño| {3, Pepe, true}|\n",
      "|      45|  #VivaElLunes|{15, Ana, false}|\n",
      "|     100|¡Gol de Señor!|  {2, Eva, true}|\n",
      "+--------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('../../data/Cap7/tweets.json') # aca no le pasamos parametros, y que spark por defecto los interpreta!\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|En el anterior ap...|\n",
      "|y que se crean me...|\n",
      "|antes de poder co...|\n",
      "|un RDD (o varios)...|\n",
      "|Estos RDDs se cre...|\n",
      "|o a partir de fic...|\n",
      "|creará un RDD y s...|\n",
      "|diferentes proces...|\n",
      "|En el anterior ap...|\n",
      "|y que se crean me...|\n",
      "|antes de poder co...|\n",
      "|un RDD (o varios)...|\n",
      "|Estos RDDs se cre...|\n",
      "|o a partir de fic...|\n",
      "|creará un RDD y s...|\n",
      "|diferentes proces...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lee todos los ficheros, y cada línea es una fila del DataFrame.\n",
    "## El DataFrame tiene una única columna 'value' de tipo string\n",
    "df = spark.read.text('../../data/Cap7/*.txt')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|En el anterior ap...|\n",
      "|En el anterior ap...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lee todos los ficheros, y cada ficheros es una fila del DataFrame\n",
    "## El DataFrame tiene una única columna 'value' de tipo string\n",
    "df = spark.read.text('../../data/Cap7/*.txt', wholetext=True)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenaiento de Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escritura de un DataFrame de 50 elementos en la carpeta /tmp/csv\n",
    "l = [('a',3.14)] * 50\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.printSchema()\n",
    "df.write.csv('/tmp/csv', header=True, mode='overwrite') # recordemos quese guardara al inicio de nuestro disco!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "si queiremos cambiar el parametro de separacion podemosa usar `sep` y para incluir las cabeceras en cada fichero utilizariamos `header=True`\n",
    "\n",
    "Otro paraetro interesante es el modo de escritura `mode`, que nos permitira congfgurar que hacer si la carpeta ya existe. Este acepta 4 valores, `append` que agregara los datos a los ficheros ya existentes, `overwrite` los sobreescribiremos completramente, `ignore` evitaremos cualquier escritura si existen ya ficheros, y `error` lanzaremos una excepecion si la carpeta ya existia anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El metodo spark.read nos permite pasar como parametro una carpeta, por lo cual no sera un problema quese escriban varios ficheros al tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del DataFrame anterior desde la carpeta /tmp/csv\n",
    "df = spark.read.csv('/tmp/csv', header=True, inferSchema=True) # todos los elmentos de esa carpeta\n",
    "df.printSchema()\n",
    "df.show(4)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "l = [('a',3.14)] * 50\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.printSchema()\n",
    "print(df.rdd.getNumPartitions())\n",
    "df.write.json('/tmp/json', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- value: double (nullable = true)\n",
      "\n",
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "|  a| 3.14|\n",
      "+---+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura del DataFrame anterior desde la carpeta /tmp/json\n",
    "df = spark.read.json('/tmp/json') # Aqui no debemos\n",
    "df.printSchema()\n",
    "df.show(4)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear un Dataframe de Pandas a partir de un Dataframe de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install distutils\n",
    "# Volcar un DataFrame a Pandas\n",
    "l = [('a',3.14)] * 12\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df_pandas = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames y MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder cargar y salvar DataFrames en MongoDB es necesario lanzar ```pyspark``` configurando el conector de Mongo-Spark. Para ello es necesario pasar el siguiente parámetro a la hora de invocar a ```pyspark```:\n",
    "\n",
    "Iiciar mongo:\n",
    "```shell\n",
    "sudo systemctl start mongod\n",
    "```\n",
    "\n",
    "verifica que esta activo:\n",
    "\n",
    "```shell\n",
    "sudo systemctl status mongod\n",
    "```\n",
    "\n",
    "empieza a usarlo:\n",
    "\n",
    "```shell\n",
    "mongosh\n",
    "```\n",
    "\n",
    "si acabas peude finaizar con:\n",
    "\n",
    "```shell\n",
    "sudo systemctl stop mongod\n",
    "```\n",
    "\n",
    "\n",
    "mongod db documentacion: [aqui](https://www.mongodb.com/docs/manual/tutorial/install-mongodb-on-ubuntu/)\n",
    "\n",
    "\n",
    "```\n",
    "pyspark --packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.2 --master local[*]\n",
    "```\n",
    "        \n",
    "El nombre del conector corresponde a _coordenadas_ Maven:\n",
    " * **2.11** porque el conector utiliza la versión 2.11 de Scala\n",
    " * **2.2.2** es la versión concreta del conector. La versión 2.2.2 soporta MongoDB 2.2.x y 2.3.x\n",
    " \n",
    "El parámetro ```--master``` es el usual. En este caso lanzamos ```pyspark``` en modo local y utilizando tantos procesos _workers_ como núcleos tenga nuestra CPU.\n",
    "\n",
    "Se puede encontrar más información en:\n",
    " * https://docs.mongodb.com/spark-connector/master/\n",
    " * https://docs.mongodb.com/spark-connector/master/python-api/\n",
    " \n",
    "**Supondremos que existe un servidor MongoDB ejecutándose en local (IP 127.0.0.1) y escuchando en el puerto por defecto (27017).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|   nombre|habitantes|\n",
      "+---------+----------+\n",
      "|   Madrid|   3182981|\n",
      "|Barcelona|   1620809|\n",
      "| Valencia|    787808|\n",
      "|  Sevilla|    689434|\n",
      "| Zaragoza|    664938|\n",
      "|   Málaga|    569002|\n",
      "|   Murcia|    443243|\n",
      "|    Palma|    406492|\n",
      "+---------+----------+\n",
      "\n",
      "root\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- habitantes: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o298.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: com.mongodb.spark.sql.DefaultSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.spark.sql.DefaultSource.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m ciudades\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      6\u001b[0m ciudades\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m      8\u001b[0m (\u001b[43mciudades\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.mongodb.spark.sql.DefaultSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m              \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongodb://127.0.0.1/test.ciudades\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# *Importante*: La colección test.ciudades no debe existir. Si lo que queremos es añadir a una colección ya existente, debemos usar mode(\"append\"):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m### ciudades.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\").mode(\"append\").save()\u001b[39;00m\n",
      "File \u001b[0;32m~/Documentos/PROGRAMACION/BIG DATA/env/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/Documentos/PROGRAMACION/BIG DATA/env/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documentos/PROGRAMACION/BIG DATA/env/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documentos/PROGRAMACION/BIG DATA/env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o298.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: com.mongodb.spark.sql.DefaultSource. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:863)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:257)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: com.mongodb.spark.sql.DefaultSource.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "# Volcar un DataFrame a una colección MongoDB\n",
    "ciudades = spark.createDataFrame([(\"Madrid\",  3182981), (\"Barcelona\", 1620809), (\"Valencia\",787808),\n",
    "                                  (\"Sevilla\", 689434), (\"Zaragoza\", 664938), (\"Málaga\",569002), \n",
    "                                  (\"Murcia\",443243), (\"Palma\",406492) ], [\"nombre\", \"habitantes\"])\n",
    "ciudades.show()\n",
    "ciudades.printSchema()\n",
    "\n",
    "(ciudades.write.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "              .option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\").save()\n",
    ")\n",
    "# *Importante*: La colección test.ciudades no debe existir. Si lo que queremos es añadir a una colección ya existente, debemos usar mode(\"append\"):\n",
    "### ciudades.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Cargar un DataFrame desde una colección MongoDB\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\").load()\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Cargar un DataFrame desde una colección MongoDB heterogénea (distintos documentos tienen atributos diferentes)\n",
    "from pymongo import MongoClient #pip install pymongo\n",
    "\n",
    "# Borramos la colección test.usuarios e insertamos dos documentos heterogéneos\n",
    "client = MongoClient('127.0.0.1')\n",
    "col = client['test']['usuarios']\n",
    "col.drop()\n",
    "col.insert_many([{'nombre':'ana', 'edad':33}, {'nombre':'pedro','altura':150}])\n",
    "\n",
    "# Cargamos un DataFrame a partir de la colección test.usuarios\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\"mongodb://127.0.0.1/test.usuarios\").load()\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pipeline = \"{'$match': {'habitantes': {$gt:500000}}}\"\n",
    "masde500mil = (spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\n",
    "                        .option(\"uri\",\"mongodb://127.0.0.1/test.ciudades\")\n",
    "                        .option(\"pipeline\", pipeline).load()\n",
    "              )\n",
    "masde500mil.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspección de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# \n",
    "l = [('a',3.14)] * 12\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.describe().show()\n",
    "type(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "l = [('a',3.14), ('b',2.0), ('c',4.5)]\n",
    "df = spark.createDataFrame(l, ['id','value'])\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "df.describe(['Age','Fare','Sex','Cabin']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrado de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Eliminación de columnas con drop()\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.columns)\n",
    "df2 = titanic.drop('PassengerId','Name','Cabin')\n",
    "print(df2.columns)\n",
    "df3 = titanic.drop(titanic.PassengerId)\n",
    "print(df3.columns)\n",
    "df4 = titanic.drop(titanic['PassengerId'])\n",
    "print(df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Selección de columnas con select()\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.columns)\n",
    "df2 = titanic.select('Survived', 'Pclass', 'Age')\n",
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.count())\n",
    "df = titanic.dropDuplicates()\n",
    "print(df.count())\n",
    "df = titanic.dropDuplicates(['Sex'])\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "print(titanic.count())\n",
    "df = titanic.dropna()\n",
    "print(df.count())\n",
    "df = titanic.drop('Cabin').dropna()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "df = titanic.filter( 'Survived = 1')\n",
    "print(df.count())\n",
    "df = titanic.filter( df.Survived == 1)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df = titanic.filter( 'Survived = 1 AND Sex = \"female\" AND Age > 20')\n",
    "print(df.count())\n",
    "df = titanic.filter( (df.Survived == 1) & (df['Sex'] == 'female') & (df.Age > 20))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Filas cuyo camarote tiene 3 caracteres, el primero es una A o una B mayúsculas y el último un 2\n",
    "# Usa una expresión regular\n",
    "df = titanic.filter('Cabin RLIKE \"^[AB].2$\"')\n",
    "print(df.count())\n",
    "df = titanic.filter(df.Cabin.rlike('^[AB].2$'))\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.union(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(3.0,'marta'),(1.0,'ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "df = df1.union(df2) ## No hay problema: long y double son compatibles => double\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([('3','marta'),('1','ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "df = df1.union(df2) ## No hay problema: long y string son compatibles => string\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(True,'marta'),(False,'ana')],['id','nombre'])\n",
    "df2.printSchema()\n",
    "## df = df1.union(df2) ## Excepcion\n",
    "##AnalysisException: \"Union can only be performed on tables with the compatible column types. boolean <> bigint at the first column of the second table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df1.printSchema()\n",
    "df2 = spark.createDataFrame([(3,'marta',33),(4,'señor',44)],['id','nombre','edad'])\n",
    "df2.printSchema()\n",
    "## df = df1.union(df2)  ## Excepción\n",
    "## AnalysisException: \"Union can only be performed on tables with the same number of columns, but the first table has 2 columns and the second table has 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.intersect(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "df2 = spark.createDataFrame([(3,'marta'),(1,'ana')],['id','nombre'])\n",
    "df = df1.subtract(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## inner join de dos DataFrames sobre la columna 'id'\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.show()\n",
    "df = users.join(age,'id')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Inner join donde la columna 'id' contiene enteros y cadenas de texto\n",
    "users = spark.createDataFrame([('1','ana'),('2','jose')],['id','nombre'])\n",
    "users.printSchema()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.printSchema()\n",
    "df = users.join(age,'id') ## No hay problema: string y long son compatibles => string\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## inner join considerando igualdad de varias columnas a la vez\n",
    "users = spark.createDataFrame([(1,'ana','golf'),(2,'jose','polo',)],['id','nombre','deporte'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,'eva',33),(2,'jose',30)],['id','nombre','edad'])\n",
    "age.show()\n",
    "df = users.join(age,[\"id\", \"nombre\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## inner join usando una expresión de igualdad entre 2 columnas\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['ident','edad'])\n",
    "age.show()\n",
    "df = users.join(age,users.id == age.ident)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## inner join usando una expresión de igualdad entre 4 columnas\n",
    "users = spark.createDataFrame([(1,'ana','golf'),(2,'jose','polo',)],['id','name','sport'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,'eva',33),(2,'jose',30)],['ident','nombre','edad'])\n",
    "age.show()\n",
    "df = users.join(age,(users.id == age.ident) & (users.name == age.nombre))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## inner join usando una expresión compleja entre 2 columnas\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(None,1,33),(2,5,30)],['id1','id2','edad'])\n",
    "age.show()\n",
    "cond = ((age.id1.isNotNull() & (users.id == age.id1)) | \n",
    "       (age.id1.isNull() & (users.id == age.id2)))\n",
    "df = users.join(age,cond)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## left outer join de users y age\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose'),(3,'eva')],['id','nombre'])\n",
    "users.show()\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.show()\n",
    "df = users.join(age,'id','left_outer')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def load_titanic():\n",
    "    return spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True).drop('Cabin').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Operaciones aritméticas entre columnas\n",
    "titanic = load_titanic()\n",
    "titanic.selectExpr(\"Survived\",\"SibSp + Parch As Family\", \"Age * 12 AS Age\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "def sex_to_num(s):\n",
    "    ret = None\n",
    "    if s == 'female':\n",
    "        ret = 0\n",
    "    elif s == 'male':\n",
    "        ret = 1\n",
    "    return ret\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "spark.udf.register(\"sex_to_num\", sex_to_num, IntegerType())\n",
    "\n",
    "titanic.selectExpr(\"Sex\", \"sex_to_num(Sex) AS Sex_num\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "spark.udf.register(\"max_int\", max, IntegerType())\n",
    "    \n",
    "titanic.selectExpr(\"SibSp\", \"Parch\", \"max_int(SibSp, Parch) AS Max_Family\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Funciones definidas por el usuario (UDF) para transformar columnas\n",
    "titanic = load_titanic()\n",
    "\n",
    "def scale(n,minv,maxv):\n",
    "    return (n - minv) / (maxv - minv)\n",
    "\n",
    "# Se puede obtener los valores minimos y maximos de cada columna a través del DataFrame generado por describe()\n",
    "summary = titanic.describe().toPandas()\n",
    "min_age = float(summary.loc[3,'Age'])\n",
    "max_age = float(summary.loc[4,'Age'])\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "spark.udf.register(\"scale_Age\", lambda x: scale(x, min_age, max_age), DoubleType())\n",
    "\n",
    "titanic.selectExpr(\"Age\", \"scale_Age(Age) AS Scaled_Age\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Transformación de columnas usando expresiones entre columnas\n",
    "titanic = load_titanic()\n",
    "titanic.select(titanic.Survived,(titanic.SibSp + titanic.Parch).alias(\"Family\"), (titanic.Age * 12).alias(\"Age\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Transformaciones con UDFs usando expresiones entre columnas\n",
    "from pyspark.sql.functions import udf\n",
    "titanic = load_titanic()\n",
    "\n",
    "sex_to_num_UDF = udf(sex_to_num, IntegerType())\n",
    "\n",
    "max_int_UDF = udf(max, IntegerType())\n",
    "\n",
    "summary = titanic.describe().toPandas()\n",
    "min_age = float(summary.loc[3,'Age'])\n",
    "max_age = float(summary.loc[4,'Age'])\n",
    "scale_Age_UDF = udf(lambda x : scale(x, min_age, max_age), DoubleType())\n",
    "                   \n",
    "titanic.select(scale_Age_UDF(titanic.Age).alias(\"Scaled_Age\"), \n",
    "               sex_to_num_UDF(titanic.Sex).alias(\"Sex_Num\"),\n",
    "               max_int_UDF(titanic.SibSp,titanic.Parch).alias(\"Max_Family\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Una única agregación\n",
    "titanic = load_titanic()\n",
    "\n",
    "titanic.groupBy().count().show()\n",
    "titanic.groupBy().sum('Survived').show()\n",
    "titanic.groupBy('Pclass').sum('Survived').show()\n",
    "titanic.groupBy('Pclass','Embarked').sum('Survived').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Varias agregaciones del mismo tipo\n",
    "titanic = load_titanic()\n",
    "\n",
    "titanic.groupBy('Pclass').sum('Survived', 'Fare').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Varias funciones de agregación diferentes a la vez\n",
    "titanic = load_titanic()\n",
    "\n",
    "# Usando diccionarios\n",
    "titanic.groupBy('Pclass').agg({'*':'count', 'Survived':'sum'}).show()\n",
    "\n",
    "# Usando una secuencia de funciones de la biblioteca pyspark.sql.functions\n",
    "from pyspark.sql import functions\n",
    "titanic.groupBy('Pclass').agg(functions.count('*').alias('Total'), functions.sum('Survived').alias('Survivors')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL sobre DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Ejecución de código SQL que combina DataFrames\n",
    "users = spark.createDataFrame([(1,'ana'),(2,'jose')],['id','nombre'])\n",
    "users.createOrReplaceTempView(\"users\")\n",
    "age = spark.createDataFrame([(1,36),(2,30)],['id','edad'])\n",
    "age.createOrReplaceTempView(\"age\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT *\n",
    "             FROM users INNER JOIN age ON users.id == age.id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "titanic = load_titanic()\n",
    "titanic.createOrReplaceTempView(\"titanic\")\n",
    "spark.sql(\"\"\"SELECT Survived, SibSp+Parch AS Family, sex_to_num(Sex) AS Sex_Num\n",
    "             FROM titanic \n",
    "             WHERE Age > 50\n",
    "             \"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Ejecución de código SQL con agregación y ordenación\n",
    "titanic = load_titanic()\n",
    "titanic.createOrReplaceTempView(\"titanic\")\n",
    "spark.sql(\"\"\"SELECT Pclass, \n",
    "                    COUNT(*) AS Total, \n",
    "                    SUM(Survived) AS Survivors \n",
    "             FROM titanic \n",
    "             GROUP BY Pclass\n",
    "             ORDER BY Pclass ASC\n",
    "             \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación con LinearSVC usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "clf = LinearSVC(featuresCol='features', labelCol='Survived')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, clf])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para clasificar 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('prediction', 'Survived')\n",
    "results.show(5)\n",
    "\n",
    "\n",
    "# Evaluación de la clasificación usando la clase experimental MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "claseval = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='Survived', metricName='accuracy')\n",
    "print('Score:', claseval.evaluate(prediction))\n",
    "\n",
    "\n",
    "# Evaluación de la regresión utilizando mllib (obsolescente)\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "rdd = results.rdd.map(lambda row: (row[0], float(row[1]))) # Es necesario representar la clase Survived como float\n",
    "                                                           # o MulticlassMetrics fallará\n",
    "metrics = MulticlassMetrics(rdd)\n",
    "print(\"Score:\", metrics.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión con LinearRegression usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "reg = LinearRegression(featuresCol='features', labelCol='Fare')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, reg])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para inferir tarifas a partir de las instancias en 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)\n",
    "\n",
    "# Evaluación utilizando la clase experimental RegressionEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "maeeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='mae')\n",
    "print(\"MAE :\", maeeval.evaluate(results))\n",
    "mseeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='mse')\n",
    "print(\"MSE :\", mseeval.evaluate(results))\n",
    "rmseeval = RegressionEvaluator(predictionCol='Prediction', labelCol='Fare', metricName='rmse')\n",
    "print(\"RMSE:\", rmseeval.evaluate(results),'\\n')\n",
    "\n",
    "# Evaluación de la regresión utilizando mllib (obsolescente)\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "rdd = results.rdd\n",
    "metrics = RegressionMetrics(rdd)\n",
    "print(\"MAE :\", metrics.meanAbsoluteError)\n",
    "print(\"MSE :\", metrics.meanSquaredError)\n",
    "print(\"RMSE:\", metrics.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de grupos con k-means usando un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Descartamos las columnas que no nos interesan y eliminamos filas con valores vacíos\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "# One-hot-encoding de la columna Embarked\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "# Combina todas las columnas en un único vector\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE','Fare'], outputCol='features_raw')\n",
    "# Normaliza los valores de cada posición del vector\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "# Realiza análisis de grupos\n",
    "clu = KMeans(k=3) # valores por defecto: atributos en 'features' y centroide en 'prediction'\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, clu])\n",
    "\n",
    "\n",
    "# Entrenamos el pipeline\n",
    "model = pipeline.fit(titanic)\n",
    "prediction = model.transform(titanic)\n",
    "prediction.select('prediction').show(5)\n",
    "\n",
    "# Obtenemos los centroides desde el último modelo del pipiline\n",
    "print('Centroides:')\n",
    "print(type(model.stages[-1].clusterCenters()[0]))\n",
    "for c in model.stages[-1].clusterCenters():\n",
    "    print(c)\n",
    "\n",
    "# Evaluación utilizando la clase experimental ClusteringEvaluator para obtener el coeficiente de silueta\n",
    "evaluator = ClusteringEvaluator()\n",
    "print('Silhouette Coefficient:', evaluator.evaluate(prediction))\n",
    "\n",
    "# No existen métodos de evaluación de clústers en mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistencia de modelos en Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Partimos en train (80%) + test (20%)\n",
    "titanic = spark.read.csv('../../data/Cap7/titanic.csv', header=True, inferSchema=True)\n",
    "titanic = titanic.drop('PassengerId', 'Name', 'Ticket','Cabin')\n",
    "titanic = titanic.dropna()\n",
    "\n",
    "train, test = titanic.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Transforma Sex y Age a valores numéricos (orden alfabético)\n",
    "indexerSex = StringIndexer(inputCol='Sex', outputCol='Sex_num', stringOrderType='alphabetAsc')\n",
    "indexerEmbarked = StringIndexer(inputCol='Embarked', outputCol='Embarked_num', stringOrderType='alphabetAsc')\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=['Embarked_num'],outputCols=['Embarked_OHE'])\n",
    "vec = VectorAssembler(inputCols=['Pclass', 'Sex_num', 'Age', 'SibSp', 'Parch', 'Survived', 'Embarked_OHE'], outputCol='features_raw')\n",
    "sca = MinMaxScaler(inputCol='features_raw', outputCol='features')\n",
    "reg = LinearRegression(featuresCol='features', labelCol='Fare')\n",
    "\n",
    "pipeline = Pipeline(stages=[indexerSex, indexerEmbarked, ohe, vec, sca, reg])\n",
    "\n",
    "# Entrenamos el pipeline y lo usamos para inferir tarifas a partir de las instancias en 'test'\n",
    "model = pipeline.fit(train)\n",
    "prediction = model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)\n",
    "\n",
    "# Volcado del modelo\n",
    "model.save('../../data/Cap7/regression_model') # Equivalente a write().save(path)\n",
    "#model.write().overwrite().save('../../data/Cap7/regression_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con 'Python 3.11.9' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/usr/local/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Carga del modelo\n",
    "loaded_model = PipelineModel.load('../../data/Cap7/regression_model')\n",
    "\n",
    "prediction = loaded_model.transform(test)\n",
    "results = prediction.select('Prediction', 'Fare')\n",
    "results.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
