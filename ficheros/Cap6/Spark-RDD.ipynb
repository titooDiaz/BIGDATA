{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCCION!\n",
    "En este capitulo vamos a ver como procesar grandes cantidades de informacion! para esto vamos a usar Apache Spark, un sistema que nos permitira almacenar y procesar grandes cantidades de datos de manera totalmente distributiva\n",
    "\n",
    "Apache spar es un sistema de computo masivo dise;ado para procesar datos de manera distributiva sobre clusteres de ordenadores. Gracias a su diseno distributivo, Spark puede procesar cantidades de datos del orden de terabytes  o incluso petabytes\n",
    "\n",
    "Apache Spark esta implementado en el lenguaje de programacion Scala, que combina los paradigmas imperativo y funcional y es ejecutado en la maquina virtual de Java.\n",
    "\n",
    "Ademas de esto ofrece APIs para Java, Python y R.\n",
    "\n",
    "\n",
    "El nucleo de Apache Spark esta dividido en:\n",
    "\n",
    "Spark SQL: Simplifica el manejo de datos masivos, Puede usar sintaxis SQL\n",
    "\n",
    "Spark Straming: Datos que se deben procesar en tiempo real\n",
    "\n",
    "Mlib: Alberga todo lo relacionado con el aprendizaje automatico\n",
    "\n",
    "GraphX: Manejo de Grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuntos de datos distribuidos resilientes (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Mi aplicación de Spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD de 5 enteros\n",
    "r = sc.parallelize([1,2,3,4,5])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD de 3 cadenas de texto\n",
    "r = sc.parallelize([\"hola\", \"hi\", \"ciao\"])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD con los primeros 100 cuadrados perfectos\n",
    "r = sc.parallelize([i*i for i in range(1,101)])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En el anterior apartado hemos explicado que los RDDs son inmutables', 'y que se crean mediante transformaciones de otros RDDs. Sin embargo,', 'antes de poder comenzar la secuencia de transformaciones necesitamos', 'un RDD (o varios) con los valores iniciales que queremos procesar.', 'Estos RDDs se crearán a partir de valores en memoria del nodo driver,', 'o a partir de ficheros accesibles desde el clúster. En ambos casos se', 'creará un RDD y sus distintas particiones se distribuirán entre los', 'diferentes procesos executor.'] ,\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:128)\r\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:291)\r\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:202)\r\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2142)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:276)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mcollect(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m r \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/Cap6/*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \u001b[38;5;66;03m# Carga todos los ficheros *.txt del directorio\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m r \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../data/Cap6\u001b[39m\u001b[38;5;124m\"\u001b[39m)           \u001b[38;5;66;03m# Carga todos los ficheros del directorio\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mcollect(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PERSONAL\\OneDrive\\Documentos\\PROGRAMACION GITHUB\\BIG DATA PYTHON\\env\\lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\PERSONAL\\OneDrive\\Documentos\\PROGRAMACION GITHUB\\BIG DATA PYTHON\\env\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\PERSONAL\\OneDrive\\Documentos\\PROGRAMACION GITHUB\\BIG DATA PYTHON\\env\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.Globber.listStatus(Globber.java:128)\r\n\tat org.apache.hadoop.fs.Globber.doGlob(Globber.java:291)\r\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:202)\r\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2142)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:276)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "r = sc.textFile(\"../../data/Cap6/file.txt\")  # Carga un fichero\n",
    "print(r.collect(),',\\n')\n",
    "r = sc.textFile(\"../../data/Cap6/*.txt\")     # Carga todos los ficheros *.txt del directorio\n",
    "print(r.collect(),'\\n')\n",
    "r = sc.textFile(\"../../data/Cap6\")           # Carga todos los ficheros del directorio\n",
    "print(r.collect(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('file:/home/kike/svn/rae/libro_Big_Data_Python/version_final/data/Cap6/file2.txt', 'En el anterior apartado hemos explicado que los RDDs son inmutables\\ny que se crean mediante transformaciones de otros RDDs. Sin embargo,\\nantes de poder comenzar la secuencia de transformaciones necesitamos\\nun RDD (o varios) con los valores iniciales que queremos procesar.\\nEstos RDDs se crearán a partir de valores en memoria del nodo driver,\\no a partir de ficheros accesibles desde el clúster. En ambos casos se\\ncreará un RDD y sus distintas particiones se distribuirán entre los\\ndiferentes procesos executor.\\n'), ('file:/home/kike/svn/rae/libro_Big_Data_Python/version_final/data/Cap6/file.txt', 'En el anterior apartado hemos explicado que los RDDs son inmutables\\ny que se crean mediante transformaciones de otros RDDs. Sin embargo,\\nantes de poder comenzar la secuencia de transformaciones necesitamos\\nun RDD (o varios) con los valores iniciales que queremos procesar.\\nEstos RDDs se crearán a partir de valores en memoria del nodo driver,\\no a partir de ficheros accesibles desde el clúster. En ambos casos se\\ncreará un RDD y sus distintas particiones se distribuirán entre los\\ndiferentes procesos executor.\\n')]\n"
     ]
    }
   ],
   "source": [
    "r = sc.wholeTextFiles(\"../../data/Cap6/*.txt\")\n",
    "print(r.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([1,2,3,4,5,6])\n",
    "print( type(r.collect()) )\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[999, 998, 997, 996, 995, 994, 993, 992, 991, 990]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.takeOrdered(10, lambda x : -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[450, 991, 1, 948, 403, 606, 992, 33, 531, 736]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.takeSample(True, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def suma(x, y):\n",
    "    return x + y\n",
    "\n",
    "r = sc.parallelize(range(1,6))\n",
    "r.reduce(suma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1,6))\n",
    "r.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiplica_positivos(x, y):\n",
    "    if x > 0 and y > 0:\n",
    "        return x*y\n",
    "    elif x > 0:\n",
    "        return x\n",
    "    elif y > 0:\n",
    "        return y\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "r = sc.parallelize([-1,2,1,-5,8], 1)\n",
    "r.reduce(multiplica_positivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.parallelize([])\n",
    "# r.reduce(lambda x,y : x+y) # Esta acción generaría una excepción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([1])\n",
    "r.reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "r = sc.parallelize(range(3), 1)  # una sola particion\n",
    "print(r.reduce(lambda x,y: x-y))\n",
    "r = sc.parallelize(range(3), 2)  # dos particiones\n",
    "print(r.reduce(lambda x,y: x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([\"hola\", \"hi\", \"ciao\"])\n",
    "r.aggregate(0, lambda c, s : c + s.count('h'), lambda c1, c2: c1 + c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.parallelize(range(1000), 2)   # 2 particiones\n",
    "r.saveAsTextFile(\"file:///tmp/file\") # Crea dos ficheros part-00000 y part-00001\n",
    "# *FALLA si /tmp/file ya existe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.parallelize(range(1000), 1)    # 1 partición\n",
    "r.saveAsTextFile(\"file:///tmp/unique\") # Crea un único fichero part-00000 con todos los números\n",
    "# *FALLA si /tmp/unique ya existe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.parallelize([\"hola\", \"hi\", \"ciao\"])\n",
    "#r.saveAsTextFile(\"file:///tmp/file\") # lanza una excepción 'Output directory file:/tmp/file already exists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([1,2,3,4])\n",
    "r2 = r.map(lambda x: x + 1)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def incrementa(x):\n",
    "    return x + 1\n",
    "\n",
    "r = sc.parallelize([1,2,3,4])\n",
    "r2 = r.map(incrementa)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([\"hola\",\"hi\",\"ciao\"])\n",
    "r2 = r.map(lambda s: len(s))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '5', '7'], ['8', '2', '4']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "r = sc.parallelize([\"1,5,7\",\"8,2,4\"])\n",
    "r2 = r.map(lambda s: list(csv.reader([s]))[0])\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5, 7], [8, 2, 4]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3 = r2.map(lambda l: [int(e) for e in l])\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '5', '7', '8', '2', '4']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "r = sc.parallelize([\"1,5,7\",\"8,2,4\"])\n",
    "r2 = r.flatMap(lambda s: list(csv.reader([s]))[0])\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def es_primo(x):\n",
    "    for i in range(2,x):\n",
    "        if x % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "r = sc.parallelize(range(2,31))\n",
    "r2 = r.filter(es_primo)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([('a',0), ('b', 1),('c',2)])\n",
    "r2 = r.mapValues(lambda x: x+1)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([('a',0), ('b', 1),('c',2)])\n",
    "r2 = r.map(lambda p: (p[0], p[1]+1))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', '1'), ('a', '5'), ('a', '7'), ('b', '8'), ('b', '2')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "r = sc.parallelize([('a','1,5,7'),('b','8,2')])\n",
    "r2 = r.flatMapValues(lambda x: list(csv.reader([x]))[0])\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x7fd36d22a898>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x7fd36d22aba8>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([('a',3.14), ('b', 9.4),('a',2.7)])\n",
    "r2 = r.groupByKey()\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [3.14, 2.7]), ('b', [9.4])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para mostrar sus elementos transformamos los iterables en listas\n",
    "r3 = r2.mapValues(lambda x: list(x))\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5), ('b', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([('a',2), ('b', 1),('a',3)])\n",
    "r2 = r.reduceByKey(lambda x,y: x+y)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 2, 4, 6]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([1,2,3,4])\n",
    "r2 = sc.parallelize([2,4,6])\n",
    "r3 = r1.union(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 6]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos duplicados con distinct()\n",
    "r4 = r3.distinct()\n",
    "r4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([1,2,3,4])\n",
    "r2 = sc.parallelize([2,4,2,6])\n",
    "r3 = r1.intersection(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 3, 4]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([1,2,3,4,1])\n",
    "r2 = sc.parallelize([2,6])\n",
    "r3 = r1.subtract(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b'), (3, 'a'), (3, 'b')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([1,2,3])\n",
    "r2 = sc.parallelize(['a','b'])\n",
    "r3 = r1.cartesian(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (2, 8))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([('a',1),('b',2),('c',3)])\n",
    "r2 = sc.parallelize([('b',8),('d',7)])\n",
    "r3 = r1.join(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (2, 8)), ('b', (2, 0))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reunión en presencia de claves duplicadas\n",
    "r1 = sc.parallelize([('a',1),('b',2),('c',3)])\n",
    "r2 = sc.parallelize([('b',8),('d',7),('b',0)])\n",
    "r3 = r1.join(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de procesamiento de RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias totales: 891\n",
      "Instancias completas: 712 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "############################################################################\n",
    "## Cargar el fichero CSV original y almacenarlo en un RDD de diccionarios ##\n",
    "############################################################################\n",
    "\n",
    "# raw: RDD de listas de cadenas de texto\n",
    "raw = (\n",
    "  sc.textFile(\"../../data/Cap6/titanic.csv\") # Leemos el fichero de texto\n",
    "    .map(lambda s: list(csv.reader([s]))[0]) # Dividimos el CSV en listas\n",
    "    .filter(lambda l: l[0] != 'PassengerId') # Eliminamos la cabecera\n",
    ")\n",
    "print(\"Instancias totales:\", raw.count())\n",
    "\n",
    "\n",
    "# Función que comprueba si una lista tiene todos los valores en las columnas que nos interesan ([1,2,4,5,6,7,9,11])\n",
    "def complete(l):\n",
    "    for i in [1,2,4,5,6,7,9,11]: # Únicamente comprobamos los campos que conservaremos\n",
    "        if l[i] == '':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Función que transforma la lista de cadenas de texto en diccionarios con enteros, reales o cadenas de texto,\n",
    "# dependiendo de la columna. Además, descarta las columnas 'PassengerId', 'Name', 'Ticket' y 'Cabin'\n",
    "def proyect_and_parse(l):\n",
    "    return {'Survived':int(l[1]), 'Pclass': int(l[2]), 'Sex':l[4], 'Age':float(l[5]), 'SibSp':int(l[6]), \n",
    "             'Parch':int(l[7]), 'Fare':float(l[9]), 'Embarked':l[11]\n",
    "           }\n",
    "\n",
    "# non_null: RDD de diccionarios con valores del tipo adecuado \n",
    "non_null = (\n",
    "  raw.filter(complete)\n",
    "     .map(proyect_and_parse)\n",
    ")\n",
    "print(\"Instancias completas:\", non_null.count(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras posibles operaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Máximos: {'Survived': 1, 'Pclass': 3, 'Sex': 'male', 'Age': 80.0, 'SibSp': 5, 'Parch': 6, 'Fare': 512.3292, 'Embarked': 'S'}\n",
      "Mínimos: {'Survived': 0, 'Pclass': 1, 'Sex': 'female', 'Age': 0.42, 'SibSp': 0, 'Parch': 0, 'Fare': 0.0, 'Embarked': 'C'}\n",
      "Primera entrada:\n",
      " {'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 22.0, 'SibSp': 1, 'Parch': 0, 'Fare': 7.25, 'Embarked': 'S'}\n",
      "Primera entrada normalizada:\n",
      " {'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 0.2711736617240513, 'SibSp': 0.2, 'Parch': 0.0, 'Fare': 0.014151057562208049, 'Embarked': 'S'} \n",
      "\n",
      "Valores únicos de la columna Sex: {'female', 'male'}\n",
      "Valores únicos de la columna Embarked: {'Q', 'C', 'S'}\n",
      "Mapping para la columna Sex: {'female': 0, 'male': 1}\n",
      "Mapping para la columna Embarked: {'C': 0, 'Q': 1, 'S': 2} \n",
      "\n",
      "Primera entrada preprocesada:\n",
      " {'Survived': 0, 'Pclass': 3, 'Sex': 1, 'Age': 0.2711736617240513, 'SibSp': 0.2, 'Parch': 0.0, 'Fare': 0.014151057562208049, 'Embarked': 2}\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "## Normalizar las columnas 'Age', 'SibSp', 'Parch' y 'Fare'. ##\n",
    "###############################################################\n",
    "\n",
    "# Función que acepta dos diccionarios y combina cada clave usando la función f\n",
    "# Suponemos que ambos diccionarios tiene las mismas claves\n",
    "def combine_dicts(f, d1, d2):\n",
    "    res = dict()\n",
    "    for k in d1.keys():\n",
    "        res[k] = f(d1[k], d2[k])\n",
    "    return res\n",
    "\n",
    "# Diccionario de valores máximos para cada columna\n",
    "maximos = non_null.reduce(lambda d1, d2: combine_dicts(max, d1, d2))\n",
    "# Diccionario de valores mínimos para cada columna\n",
    "minimos = non_null.reduce(lambda d1, d2: combine_dicts(min, d1, d2))\n",
    "\n",
    "print(\"Máximos:\", maximos)\n",
    "print(\"Mínimos:\", minimos)\n",
    "\n",
    "# Función que procesa todas las claves en 'keys' de un diccionario y las\n",
    "# normaliza usando los diccionarios de valores máximos 'maxs' y mínimos 'mins'\n",
    "# El resto de entradas se queda inalterado\n",
    "def normalize(d, keys, maxs, mins):\n",
    "    res = dict()\n",
    "    for (k,v) in d.items():\n",
    "        if k in keys:\n",
    "            res[k] = (v - mins[k]) / (maxs[k] - mins[k])\n",
    "        else:\n",
    "            res[k] = v\n",
    "    return res\n",
    "\n",
    "normalized = non_null.map(lambda d: normalize(d, ['Age','SibSp','Parch','Fare'],  maximos, minimos))\n",
    "print(\"Primera entrada:\\n\",non_null.first())\n",
    "print(\"Primera entrada normalizada:\\n\",normalized.first(),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "## Transformar columnas 'Sex' y 'Embarked' a valores enteros consecutivos a partir del 0. ##\n",
    "############################################################################################\n",
    "\n",
    "# Función que actualiza el conjunto de valores únicos con el valor que contiene el diccionario en\n",
    "# una columna 'key'\n",
    "def update_set_dict(s, d, key):\n",
    "    s.add(d[key]) # Spark permite modificar el primer parámetro y devolverlo\n",
    "    return s\n",
    "\n",
    "# Diccionario de conjuntos de valores únicos para la columna 'Sex'\n",
    "sex_values = normalized.aggregate(set(), \n",
    "                            lambda s, d: update_set_dict(s, d, 'Sex'),\n",
    "                            lambda s1,s2: s1.union(s2)\n",
    "             )\n",
    "\n",
    "# Diccionario de conjuntos de valores únicos para la columna 'Embarked'\n",
    "embarked_values = normalized.aggregate(set(), \n",
    "                            lambda s, d: update_set_dict(s, d, 'Embarked'),\n",
    "                            lambda s1,s2: s1.union(s2)\n",
    "             )\n",
    "\n",
    "print(\"Valores únicos de la columna Sex:\", sex_values)\n",
    "print(\"Valores únicos de la columna Embarked:\", embarked_values)\n",
    "\n",
    "# Función que acepta un objeto iterable (lista, conjunto, ndarray...) de valores únicos y \n",
    "# devuelve un diccionario para traducir valor -> número. Los números naturales están en \n",
    "# el rango [0, len(it)) \n",
    "def values_to_mapping(it):\n",
    "    values = sorted(it)\n",
    "    size = len(values)\n",
    "    return {k:v for (k,v) in zip(values,range(size))}\n",
    "\n",
    "# Construye los diccionarios traductores a partir de los conjuntos de valores únicos\n",
    "sex_mapping = values_to_mapping(sex_values)\n",
    "embarked_mapping = values_to_mapping(embarked_values)\n",
    "print(\"Mapping para la columna Sex:\", sex_mapping)\n",
    "print(\"Mapping para la columna Embarked:\", embarked_mapping,'\\n')\n",
    "\n",
    "\n",
    "# Función que dado un diccionario y una clave, cambia su valor por su traducción según 'mapping'\n",
    "def dict_translate(d, key, mapping):\n",
    "    d[key] = mapping[d[key]]\n",
    "    return d\n",
    "\n",
    "# Traduce las entradas 'Sex' y 'Embarked' de los diccionarios por sus valores traducidos\n",
    "titanic = (normalized\n",
    "             .map(lambda d: dict_translate(d, 'Sex', sex_mapping))\n",
    "             .map(lambda d: dict_translate(d, 'Embarked', embarked_mapping))\n",
    "          )\n",
    "print(\"Primera entrada preprocesada:\\n\",titanic.first())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
