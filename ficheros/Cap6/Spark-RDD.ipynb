{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECUERDA TENER ANACONDA INSTALADA!\n",
    "pip install ipykernel\n",
    "\n",
    "python -m ipykernel install --user --name=BigData\n",
    "\n",
    "pip install py4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCCION!\n",
    "En este capitulo vamos a ver como procesar grandes cantidades de informacion! para esto vamos a usar Apache Spark, un sistema que nos permitira almacenar y procesar grandes cantidades de datos de manera totalmente distributiva\n",
    "\n",
    "Apache spar es un sistema de computo masivo dise;ado para procesar datos de manera distributiva sobre clusteres de ordenadores. Gracias a su diseno distributivo, Spark puede procesar cantidades de datos del orden de terabytes  o incluso petabytes\n",
    "\n",
    "Apache Spark esta implementado en el lenguaje de programacion Scala, que combina los paradigmas imperativo y funcional y es ejecutado en la maquina virtual de Java.\n",
    "\n",
    "Ademas de esto ofrece APIs para Java, Python y R.\n",
    "\n",
    "\n",
    "El nucleo de Apache Spark esta dividido en:\n",
    "\n",
    "Spark SQL: Simplifica el manejo de datos masivos, Puede usar sintaxis SQL\n",
    "\n",
    "Spark Straming: Datos que se deben procesar en tiempo real\n",
    "\n",
    "Mlib: Alberga todo lo relacionado con el aprendizaje automatico\n",
    "\n",
    "GraphX: Manejo de Grafos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjuntos de datos distribuidos resilientes (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 caracteristicas de los RDDS\n",
    "\n",
    "- Estan formados por un conjunto de registros, tambien llamados elementos, todos del mismo tipo. Por ejemplo, si cargamos un fichero de textro se creara un RDD de cadenas de texto, una por cada linea del fichero\n",
    "\n",
    "- Han sido creados para ser distributivos,\n",
    "\n",
    "- Son inmutables una vez que se les asigna un valor no son modificables.\n",
    "\n",
    "- Tienen mucha facilidad a recuperar su estado inicial cuando hay algun problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(\"✅\")\n",
    "conf = SparkConf().setAppName(\"MiApp\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MiguelDiaz:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MiApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=MiApp>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lamentablmenete los tipos en python no nos ofrecen mas informacion acerca del tipo de dato de RDD. Unicamente no s dira que es un RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD de 5 enteros\n",
    "# SRE CREA UN RDD DDE 5 REGISTROS CON LOS NUMEROS DEL 1 AL 5\n",
    "r = sc.parallelize([1,2,3,4,5])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD de 3 cadenas de texto\n",
    "# LO MISMO PERO CON STRINGS\n",
    "r = sc.parallelize([\"hola\", \"hi\", \"ciao\"])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien podemos usar herramientas de python para crear RDD's mas grandes y complejos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD con los primeros 100 cuadrados perfectos\n",
    "r = sc.parallelize([i*i for i in range(1,101)])\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECORDEMOS TENER INSTALADO `pip install pyspark` en windows y en nuestro entorno virtual!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1En el anterior apartado hemos explicado que los RDDs son inmutables', 'y que se crean mediante transformaciones de otros RDDs. Sin embargo,', 'antes de poder comenzar la secuencia de transformaciones necesitamos', 'un RDD (o varios) con los valores iniciales que queremos procesar.', 'Estos RDDs se crearán a partir de valores en memoria del nodo driver,', 'o a partir de ficheros accesibles desde el clúster. En ambos casos se', 'creará un RDD y sus distintas particiones se distribuirán entre los', 'diferentes procesos executor.'] ,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = sc.textFile(\"../../data/Cap6/file.txt\")  # Carga un fichero\n",
    "print(r.collect(),',\\n')\n",
    "# r = sc.textFile(\"../../data/Cap6/*.txt\")     # Carga todos los ficheros *.txt del directorio\n",
    "# print(r.collect(),',\\n')\n",
    "# r = sc.textFile(\"../../data/Cap6/\")           # Carga todos los ficheros del directorio\n",
    "# print(r.collect(),',\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = sc.wholeTextFiles(\"../../data/Cap6/*.txt\")\n",
    "# print(r.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones\n",
    "Las acciiones son impacientes, es decrir que al lanzar una accion sobre un RDD esta comenzara inmediatamente su ejecucion de manera distribuida a lo largo del cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect, Take y Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect\n",
    "nos sirve para tomar elementos RDD's con poca cantidad de datos! Nos deveulve todos los elementos del RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([1,2,3,4,5,6])\n",
    "print( type(r.collect()) )\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take\n",
    "Nos sirve para tomar elementos de RDD's extremadamente grandes, ya que solo nos devuelve los primeros `n` elementos del RDD sin colapsar el proceso driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En take existen dos metodos que nos ayudan tener en cuenta el orden concreto en que queremos ver los elementos obtenidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[999, 998, 997, 996, 995, 994, 993, 992, 991, 990]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.takeOrdered(10, lambda x : -x) # menor a mayor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[906, 331, 845, 994, 21, 309, 953, 461, 743, 432]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.takeSample(True, 10) # 10 elementos aleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count\n",
    "calcula el numero de elementos de un RDD y lo devuelve como entero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000))\n",
    "r.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce y Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la reduccion es una funcion binaria que acepta dos elementos de tipo T y devuelve un valor del mismo tipo T (TxT->T).\n",
    "\n",
    "Recibe una funcion como argumento, va a seleccionar los dos primeros elementos luego aplica la funcion, y hace esto con los elementos resultantes, hasta que quede un unico elemento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def suma(x, y):\n",
    "    return x + y\n",
    "\n",
    "r = sc.parallelize(range(1,6))\n",
    "r.reduce(suma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize(range(1,6))\n",
    "r.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multiplica_positivos(x, y):\n",
    "    if x > 0 and y > 0:\n",
    "        return x*y\n",
    "    elif x > 0:\n",
    "        return x\n",
    "    elif y > 0:\n",
    "        return y\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "r = sc.parallelize([-1,2,1,-5,8], 1)\n",
    "r.reduce(multiplica_positivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.parallelize([])\n",
    "# r.reduce(lambda x,y : x+y) # Esta acción generaría una excepción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([1]) # nos devuleve el mismo elemento\n",
    "r.reduce(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al no estar garantizado el orden, la resta no nos servira para reducir terminos, porque al cambiar las particiones el orden cambia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "r = sc.parallelize(range(3), 1)  # una sola particion\n",
    "print(r.reduce(lambda x,y: x-y))\n",
    "r = sc.parallelize(range(3), 2)  # dos particiones\n",
    "print(r.reduce(lambda x,y: x-y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EN GENERAL LA RESTA Y LA DIVISION NO SE DEBEN USAR PARA LA REDUCCION!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATE\n",
    "Supongamos que queremos contar todas las letras h que tienen nuestros elementos  de la lista. Para ello, podemos usar una funcion de spark llamada aggregate:\n",
    "\n",
    "Este aceptara 3 parametros\n",
    "\n",
    "- Un valor inicial como el acumulador (tipo t)\n",
    "\n",
    "- Una funcion sqOp para combinar elementos de nuestro RDD (tipo c)\n",
    "\n",
    "- Una funcion combOp para combinar dos acumuladores (devuelve valor tipo c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([\"hola\", \"hi\", \"ciao\"])\n",
    "r.aggregate(0, lambda c, s : c + s.count('h'), lambda c1, c2: c1 + c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvar RDD's en ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ya existe!\n"
     ]
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000), 2)   # 2 particiones\n",
    "# A veces no funciona!\n",
    "try:\n",
    "    r.saveAsTextFile(\"file:///tmp/file\") # Crea dos ficheros part-00000 y part-00001\n",
    "except:\n",
    "    print(\"Ya existe!\")\n",
    "    \n",
    "# *FALLA si /tmp/file ya existe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya existe el archivo\n"
     ]
    }
   ],
   "source": [
    "r = sc.parallelize(range(1000), 1)    # 1 partición\n",
    "try : # Crea un único fichero part-00000 con todos los números\n",
    "    r.saveAsTextFile(\"file:///tmp/unique\")\n",
    "except:\n",
    "    print(\"ya existe el archivo\")\n",
    "# *FALLA si /tmp/unique ya existe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sc.parallelize([\"hola\", \"hi\", \"ciao\"])\n",
    "#r.saveAsTextFile(\"file:///tmp/file\") # lanza una excepción 'Output directory file:/tmp/file already exists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP Y FLATMAP\n",
    "\n",
    "#### Map \n",
    "Nos creara un RDD nuevo, donde cada elemento resultante es el resultado de aplicar la funcion f a un elemento del RDD original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CON FUNCION ANONIMA\n",
    "r = sc.parallelize([1,2,3,4])\n",
    "r2 = r.map(lambda x: x + 1)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FUNCION DEFINIDA\n",
    "def incrementa(x):\n",
    "    return x + 1\n",
    "\n",
    "r = sc.parallelize([1,2,3,4])\n",
    "r2 = r.map(incrementa)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 2, 4]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LEEMOS LAS LONGITUDES DE LOS DATOS Y LOS POENEMOS\n",
    "#-----------------------------------------------------------\n",
    "r = sc.parallelize([\"hola\",\"hi\",\"ciao\"])\n",
    "r2 = r.map(lambda s: len(s))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '5', '7'], ['8', '2', '4']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LEER ARCHIVOS\n",
    "import csv\n",
    "r = sc.parallelize([\"1,5,7\",\"8,2,4\"])\n",
    "r2 = r.map(lambda s: list(csv.reader([s]))[0])\n",
    "#                                      \\_->Objeto iterable para el csv reader\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5, 7], [8, 2, 4]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CONVERTIMOS LOS NUMEROS A ENTEROS (ESTABAN COMO STRING)\n",
    "r3 = r2.map(lambda l: [int(e) for e in l])\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLATMAP\n",
    "A diferencia del map el flatMap aplana los elementos que originalmente son listas!, es decrir, un elemento que originalmente era:\n",
    "\n",
    "[1,2,3] se convertira a 1,2,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos ejemplos similares que con el map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '5', '7', '8', '2', '4']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "r = sc.parallelize([\"1,5,7\",\"8,2,4\"])\n",
    "r2 = r.flatMap(lambda s: list(csv.reader([s]))[0])\n",
    "r2.collect()\n",
    "\n",
    "#Ahora vemos que nos devuelve una unica lista! porque al aplanar la lista se quita la diferencia entre ellas, y como el FlatMap genera una lista final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "El metodo filter recibe como parametro una funcion que calcula la conducion deseada. La cual devolvera Valores Booleanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def es_primo(x):\n",
    "    for i in range(2,x):\n",
    "        if x % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "r = sc.parallelize(range(2,31))\n",
    "r2 = r.filter(es_primo)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs de parejas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([('a',0), ('b', 1),('c',2)])\n",
    "r2 = r.mapValues(lambda x: x+1)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([('a',0), ('b', 1),('c',2)])\n",
    "r2 = r.map(lambda p: (p[0], p[1]+1))\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', '1'), ('a', '5'), ('a', '7'), ('b', '8'), ('b', '2')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "#dividir \n",
    "r = sc.parallelize([('a','1,5,7'),('b','8,2')])\n",
    "r2 = r.flatMapValues(lambda x: list(csv.reader([x]))[0])\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey\n",
    "Esto hara que todos los elmetnos con la misma clave se agrupen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x1dbc8467550>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x1dbdf5deeb0>)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([('a',3.14), ('b', 9.4),('a',2.7)])\n",
    "r2 = r.groupByKey()\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [3.14, 2.7]), ('b', [9.4])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para mostrar sus elementos transformamos los iterables en listas\n",
    "r3 = r2.mapValues(lambda x: list(x))\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "lo usamos para aplicar una funcion de combinacion a todos los valores asiciados a una misma clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5), ('b', 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = sc.parallelize([('a',2), ('b', 1),('a',3)])\n",
    "r2 = r.reduceByKey(lambda x,y: x+y)\n",
    "r2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones combinando dos RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union\n",
    "Une dos RDD's, si hay datos repetidos, apareceran repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 2, 4, 6]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([1,2,3,4])\n",
    "r2 = sc.parallelize([2,4,6])\n",
    "r3 = r1.union(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct\n",
    "Si Hay elementos repetidos, aparecera unicamente uno de ellos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 1, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos duplicados con distinct()\n",
    "r4 = r3.distinct()\n",
    "r4.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection\n",
    "Selecciona unicamente los elementos repetidos entre los dos RDD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([1,2,3,4])\n",
    "r2 = sc.parallelize([2,4,2,6])\n",
    "r3 = r1.intersection(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract\n",
    "Selecciona los elementos del primer RDD que no aparecen en el segundo RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 1, 3]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([1,2,3,4,1])\n",
    "r2 = sc.parallelize([2,6])\n",
    "r3 = r1.subtract(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mas tipos de combinaciones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cartesian\n",
    "Si el primer RDD tiene N elementos y el segundo M elementos, el resultado sera un RDD con N x M parejas, una por cadda posible combinacion del perimer RDD con el otro RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b'), (3, 'a'), (3, 'b')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([1,2,3])\n",
    "r2 = sc.parallelize(['a','b'])\n",
    "#1(a,b), 2(a,b), 3(a,b)\n",
    "#[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b'), (3, 'a'), (3, 'b')]\n",
    "r3 = r1.cartesian(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join\n",
    "Creara una pareja con el valor en el primer RDD seguido del valor en el segundo RDD\n",
    "Es decir, buscara las claves iguales y las combinara!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (2, 8))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = sc.parallelize([('a',1),('b',2),('c',3)])\n",
    "r2 = sc.parallelize([('b',8),('d',7)])\n",
    "r3 = r1.join(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso como hay dos claves iguales, se crean dos elementos con esa misma clave!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (2, 8)), ('b', (2, 0))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reunión en presencia de claves duplicadas\n",
    "r1 = sc.parallelize([('a',1),('b',2),('c',3)])\n",
    "r2 = sc.parallelize([('b',8),('d',7),('b',0)])\n",
    "r3 = r1.join(r2)\n",
    "r3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de procesamiento de RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias totales: 891\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "############################################################################\n",
    "## Cargar el fichero CSV original y almacenarlo en un RDD de diccionarios ##\n",
    "############################################################################\n",
    "\n",
    "# raw: RDD de listas de cadenas de texto\n",
    "raw = (\n",
    "  sc.textFile(\"../../data/Cap6/titanic.csv\") # Leemos el fichero de texto\n",
    "    .map(lambda s: list(csv.reader([s]))[0]) # Dividimos el CSV en listas\n",
    "    .filter(lambda l: l[0] != 'PassengerId') # Eliminamos la cabecera\n",
    ")\n",
    "print(\"Instancias totales:\", raw.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones para purificar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que comprueba si una lista tiene todos los valores en las columnas que nos interesan ([1,2,4,5,6,7,9,11])\n",
    "def complete(l):\n",
    "    for i in [1,2,4,5,6,7,9,11]: # Únicamente comprobamos los campos que conservaremos\n",
    "        if l[i] == '':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Función que transforma la lista de cadenas de texto en diccionarios con enteros, reales o cadenas de texto,\n",
    "# dependiendo de la columna. Además, descarta las columnas 'PassengerId', 'Name', 'Ticket' y 'Cabin'\n",
    "def proyect_and_parse(l):\n",
    "    return {'Survived':int(l[1]), 'Pclass': int(l[2]), 'Sex':l[4], 'Age':float(l[5]), 'SibSp':int(l[6]), \n",
    "             'Parch':int(l[7]), 'Fare':float(l[9]), 'Embarked':l[11]\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar funciones definidas recientemente!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias completas: 712 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# non_null: RDD de diccionarios con valores del tipo adecuado \n",
    "non_null = (\n",
    "  raw.filter(complete)\n",
    "     .map(proyect_and_parse)\n",
    ")\n",
    "print(\"Instancias completas:\", non_null.count(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otras posibles operaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Máximos: {'Survived': 1, 'Pclass': 3, 'Sex': 'male', 'Age': 80.0, 'SibSp': 5, 'Parch': 6, 'Fare': 512.3292, 'Embarked': 'S'}\n",
      "Mínimos: {'Survived': 0, 'Pclass': 1, 'Sex': 'female', 'Age': 0.42, 'SibSp': 0, 'Parch': 0, 'Fare': 0.0, 'Embarked': 'C'}\n",
      "Primera entrada:\n",
      " {'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 22.0, 'SibSp': 1, 'Parch': 0, 'Fare': 7.25, 'Embarked': 'S'}\n",
      "Primera entrada normalizada:\n",
      " {'Survived': 0, 'Pclass': 3, 'Sex': 'male', 'Age': 0.2711736617240513, 'SibSp': 0.2, 'Parch': 0.0, 'Fare': 0.014151057562208049, 'Embarked': 'S'} \n",
      "\n",
      "Valores únicos de la columna Sex: {'female', 'male'}\n",
      "Valores únicos de la columna Embarked: {'C', 'Q', 'S'}\n",
      "Mapping para la columna Sex: {'female': 0, 'male': 1}\n",
      "Mapping para la columna Embarked: {'C': 0, 'Q': 1, 'S': 2} \n",
      "\n",
      "Primera entrada preprocesada:\n",
      " {'Survived': 0, 'Pclass': 3, 'Sex': 1, 'Age': 0.2711736617240513, 'SibSp': 0.2, 'Parch': 0.0, 'Fare': 0.014151057562208049, 'Embarked': 2}\n"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "## Normalizar las columnas 'Age', 'SibSp', 'Parch' y 'Fare'. ##\n",
    "###############################################################\n",
    "\n",
    "# Función que acepta dos diccionarios y combina cada clave usando la función f\n",
    "# Suponemos que ambos diccionarios tiene las mismas claves\n",
    "def combine_dicts(f, d1, d2):\n",
    "    res = dict()\n",
    "    for k in d1.keys():\n",
    "        res[k] = f(d1[k], d2[k])\n",
    "    return res\n",
    "\n",
    "# Diccionario de valores máximos para cada columna\n",
    "maximos = non_null.reduce(lambda d1, d2: combine_dicts(max, d1, d2))\n",
    "# Diccionario de valores mínimos para cada columna\n",
    "minimos = non_null.reduce(lambda d1, d2: combine_dicts(min, d1, d2))\n",
    "\n",
    "print(\"Máximos:\", maximos)\n",
    "print(\"Mínimos:\", minimos)\n",
    "\n",
    "# Función que procesa todas las claves en 'keys' de un diccionario y las\n",
    "# normaliza usando los diccionarios de valores máximos 'maxs' y mínimos 'mins'\n",
    "# El resto de entradas se queda inalterado\n",
    "def normalize(d, keys, maxs, mins):\n",
    "    res = dict()\n",
    "    for (k,v) in d.items():\n",
    "        if k in keys:\n",
    "            res[k] = (v - mins[k]) / (maxs[k] - mins[k])\n",
    "        else:\n",
    "            res[k] = v\n",
    "    return res\n",
    "\n",
    "normalized = non_null.map(lambda d: normalize(d, ['Age','SibSp','Parch','Fare'],  maximos, minimos))\n",
    "print(\"Primera entrada:\\n\",non_null.first())\n",
    "print(\"Primera entrada normalizada:\\n\",normalized.first(),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "## Transformar columnas 'Sex' y 'Embarked' a valores enteros consecutivos a partir del 0. ##\n",
    "############################################################################################\n",
    "\n",
    "# Función que actualiza el conjunto de valores únicos con el valor que contiene el diccionario en\n",
    "# una columna 'key'\n",
    "def update_set_dict(s, d, key):\n",
    "    s.add(d[key]) # Spark permite modificar el primer parámetro y devolverlo\n",
    "    return s\n",
    "\n",
    "# Diccionario de conjuntos de valores únicos para la columna 'Sex'\n",
    "sex_values = normalized.aggregate(set(), \n",
    "                            lambda s, d: update_set_dict(s, d, 'Sex'),\n",
    "                            lambda s1,s2: s1.union(s2)\n",
    "             )\n",
    "\n",
    "# Diccionario de conjuntos de valores únicos para la columna 'Embarked'\n",
    "embarked_values = normalized.aggregate(set(), \n",
    "                            lambda s, d: update_set_dict(s, d, 'Embarked'),\n",
    "                            lambda s1,s2: s1.union(s2)\n",
    "             )\n",
    "\n",
    "print(\"Valores únicos de la columna Sex:\", sex_values)\n",
    "print(\"Valores únicos de la columna Embarked:\", embarked_values)\n",
    "\n",
    "# Función que acepta un objeto iterable (lista, conjunto, ndarray...) de valores únicos y \n",
    "# devuelve un diccionario para traducir valor -> número. Los números naturales están en \n",
    "# el rango [0, len(it)) \n",
    "def values_to_mapping(it):\n",
    "    values = sorted(it)\n",
    "    size = len(values)\n",
    "    return {k:v for (k,v) in zip(values,range(size))}\n",
    "\n",
    "# Construye los diccionarios traductores a partir de los conjuntos de valores únicos\n",
    "sex_mapping = values_to_mapping(sex_values)\n",
    "embarked_mapping = values_to_mapping(embarked_values)\n",
    "print(\"Mapping para la columna Sex:\", sex_mapping)\n",
    "print(\"Mapping para la columna Embarked:\", embarked_mapping,'\\n')\n",
    "\n",
    "\n",
    "# Función que dado un diccionario y una clave, cambia su valor por su traducción según 'mapping'\n",
    "def dict_translate(d, key, mapping):\n",
    "    d[key] = mapping[d[key]]\n",
    "    return d\n",
    "\n",
    "# Traduce las entradas 'Sex' y 'Embarked' de los diccionarios por sus valores traducidos\n",
    "titanic = (normalized\n",
    "             .map(lambda d: dict_translate(d, 'Sex', sex_mapping))\n",
    "             .map(lambda d: dict_translate(d, 'Embarked', embarked_mapping))\n",
    "          )\n",
    "print(\"Primera entrada preprocesada:\\n\",titanic.first())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
